{
  "chunk-275ef867cb5731e444b6681c3aad3e94": {
    "tokens": 76,
    "content": "DSE is exactly the BSE, which is always less than or equal to RME. We also highlight the increasing\nper round UDSE. As shown, though the UDSE might be below than URME in the beginning rounds, it\ncatches up with URME as T increases, consistent with Theorem 4. Interestingly, the UDSE converged\n19",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_96",
    "file_path": "CSE_1.pdf"
  },
  "chunk-8a4844ad2405a122cb1211ef71c78026": {
    "tokens": 84,
    "content": "achieves a per round expected utility of 1\n2 or a total utility of 1\n2T over T rounds. However, consider\nthe following dynamic strategy without communication:\nx1 = (1, 0); xt =\n(\n(0, 1) if ∃t′ < tand jt′ = j1\n(1, 0) otherwise for t >1. (1)\n16",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_85",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e5df52521ac4156e80f4810b35baa20a": {
    "tokens": 261,
    "content": "over the rounds of the game. However, to do this, the leader may take advantage of the follower’s\nrevealed preferences [11, 51], so that she can distinguish the follower type hence tailor her strategies\naccordingly. As a byproduct of such optimization, a dynamic policy π may learn the follower type.\nFrom this perspective, DSE as the optimal dynamic Stackelberg policy is a concept that combines\nlearning with utility optimization. Formally, we define effective learning as follows:\nDefinition 1 (Effective Learning). For a dynamic Bayesian Stackelberg game {R, Θ, {Cθ}θ∈Θ,\nµ, T}, a DSP strategy π learns effectively if there exists θ, θ′ ∈ Θ and t ∈ {2, ..., T} such that for\nfollower best response histories j∗θ\nt−1 and j∗θ′\nt−1, π(j∗θ\nt−1) ̸= π(j∗θ′\nt−1) and the leader utility realized by π\nexceeds the leader utility realized by repeatedly playing the BSE leader strategy for T rounds.\nNote that the DSE is, by necessity, a dynamic policy, and we assume that the leader has full",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_45",
    "file_path": "CSE_1.pdf"
  },
  "chunk-616885dc2f8ff4c5bb01c6dabefc801a": {
    "tokens": 277,
    "content": "tions. Linear Algebra Appl. 199 (1994), 339–355.\n[4] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. 2013. Learning prices for repeated\nauctions with strategic buyers. In Advances in Neural Information Processing Systems . 1169–\n1177.\n[5] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. 2014. Repeated contextual auctions\nwith strategic buyers. In Advances in Neural Information Processing Systems . 622–630.\n[6] Nivasini Ananthakrishnan, Nika Haghtalab, Chara Podimata, and Kunhe Yang. 2024. Is\nKnowledge Power? On the (Im)Possibility of Learning from Strategic Interaction. https:\n//doi.org/10.48550/arXiv.2408.08272 arXiv:2408.08272 [cs]\n[7] Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V Vazirani. 2014.\nLearning economic parameters from revealed preferences. In International Conference on Web\nand Internet Economics . Springer, 338–353.\n[8] Santiago R. Balseiro, Anthony Kim, and Daniel Russo. 2021. On the Futility of Dynamics",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_103",
    "file_path": "CSE_1.pdf"
  },
  "chunk-46c6b369efad96a5b915ad5c04f13c32": {
    "tokens": 165,
    "content": "Note that the DSE is, by necessity, a dynamic policy, and we assume that the leader has full\ncommitment power. Therefore, even with full knowledge of the follower type, the DSE should be\nbetter than the optimal static strategy due to the commitment power of the leader. However, we\nonly consider learning to be effective when the leader employs different strategies based on divergent\nbest response histories. Strategies that adapt to distinct best response histories require learning, in\naddition to commitment. In our experiments, we generally do not, and cannot, distinguish between\nimprovements relative to static policies due to commitment versus learning. However, in Section 3\nwe demonstrate that learning alone will ensure higher utility for the leader, relative to the optimal\nstatic policy, in most dynamic Bayesian Stackelberg games.\n7",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_46",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7c5db805e79ecedd8e24b786f718923b": {
    "tokens": 249,
    "content": "(DR2) of the Dark Energy Spectroscopic Instrument (DESI) collaboration [2]. We consider a dark energy scaling\nproportional to the squared Hubble parameter H(z) as\nρDE(z) = Λ0 + ν MPlH2(z)\n16π2 , (1)\nwhere Λ0 is the contribution of the classical cosmological constant to the dark energy density ρDE(z), MPl is\nthe Planck mass, and ν is a free parameter of the model. As discussed in our previous work, this scaling is\nmotivated by the Cohen–Kaplan–Nelson (CKN) bound [3], used to connect the quantum corrections of the dark\nenergy density to the size of the universe. Hence, as introduced in Reference [1], we call the dark energy density\nevolving from Equation (1) νCKN model, or simply CKN model for the case ν = 1.\nIn Section 2, we show the result of our combined analysis of the DESI baryonic acoustic oscillations (BAO)\nDR2 [2], supernova datasets [4, 5] and model-independent Hubble measurements [6, 7]. We compare the results",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_1",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-18d90e40e84cf6a7a6e98318ae48a024": {
    "tokens": 169,
    "content": "achieve optimality in a dynamic setting with strategic agents?” Stated another way, “Generally,\ncan one both learn and exploit the information learned when facing strategic agents if we move\nbeyond the dynamic pricing problem?”\nThere are many settings outside of dynamic pricing that share similar strategic concerns,\nspecifically one party, the leader, sets a policy that induces a response by another party, the follower.\nHowever, the follower may have some private information that would inform the optimal leaders\npolicy while also having an incentive to mislead the leader due to conflicting objectives. For example,\nin contract design [13] the principal, the leader, is designing a contract to induce certain actions\nin the agent, the follower. The contract may be revisited periodically, allowing for opportunities\nfor the principal to redesign the contract given the revealed information from previous contracting",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_20",
    "file_path": "CSE_1.pdf"
  },
  "chunk-379aee77be1efcfa6af71b3b441792c0": {
    "tokens": 75,
    "content": "However, this learning does not happen in a strategic setting, which is our main focus.\nRecent work has also considered the computation [ 12, 16, 28, 60, 61] of Stackelberg equilibria in\nstochastic games (see [ 53] for a definition of stochastic games). However, in this literature, there is\n5",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_38",
    "file_path": "CSE_1.pdf"
  },
  "chunk-a33f872a4556988305e02ce770acaf84": {
    "tokens": 118,
    "content": "X\nj\npθ,jU(xθ,j, j) =\nX\nj\n(1 −\nr\nlog |Θ|\nT δ2 )p∗\nθ,jU(x∗\nθ,j, j) + (\nr\nlog |Θ|\nT δ2 )pδ\nθ,jU(xδ\nθ,j, j) (35)\n≥\nX\nj\n(1 −\nr\nlog |Θ|\nT δ2 )p∗\nθ,jU(x∗\nθ,j, j) (36)\n36",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_150",
    "file_path": "CSE_1.pdf"
  },
  "chunk-43a03f959b5a1dc7b28c742bd07c5df7": {
    "tokens": 210,
    "content": "One potential policy is to repeatedly post the Myerson price [45], p∗ = arg maxp[p(1−Pr(v ≤ p))],\nwhich maximizes the single-round revenue under seller’s prior knowledge µ. This policy would\nappear highly sub-optimal – for example, if the buyer rejects the item in the first round, he will\nreject it in all future rounds, leading to a revenue of zero. One might conjecture that the seller\nshould be able to gradually learn the exact v from repeatedly observing the buyer’s responses, and\nthen set the price to v, extracting maximum revenue in all future rounds. However, the no learning\ntheorem shows that, due to the buyer’s strategic responses to the seller’s learning, it is impossible\nfor the seller to achieve higher expected revenue using any other strategy [57]!\nTheorem 1 (No Learning Folk Theorem for the Dynamic Pricing Game). The leader utility realized\nby the repeated BSE is identical to the leader utility realized by the DSE.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_48",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7cb58f3dc3c82f46fe6b1d694ff22c17": {
    "tokens": 120,
    "content": "above example, the optimal dynamic utility is still worse than Myerson’s revenue of 32 when the\nseller has an available price of 96. Therefore, even though the no learning theorem breaks down\nin the Stackelberg game (Table 3) in the sense that the optimal dynamic policy outperforms the\noptimal static policy, it does not outperform the Myerson revenue.\nAdditionally, Example 3 does not rely on commitment directly to improve the seller’s utility. In\nthis case, the seller offers the optimal (restricted) price assuming that the buyer is the highest type,\n10",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_58",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7276f962d8d53d3aead90da59b7bf45a": {
    "tokens": 45,
    "content": "[54]; (2) the randomized menu over mixed strategies may not be plausible as a commitment due to\nthe infeasibility of verifying the two layers of randomness. Therefore, we treat it mainly as a strong\n15",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_81",
    "file_path": "CSE_1.pdf"
  },
  "chunk-d0f0899823bfcd09b76189894c197a6b": {
    "tokens": 332,
    "content": "for some set of ϵi. We call these cones the Schl¨ afli cones, following Hug and Schneider [35].\nLemma 1 (Lemma 8.2.1 in Schneider and Weil [52]). For a set of hyperplanes H1, H2, ..., Hn ∈\nG(m, m− 1) in general position, there are C(n, m) distinct Schl¨ afli Cones where\nC(n, m) = 2\nm−1X\ni=0\n\u0012n − 1\ni\n\u0013\n. (13)\nWe will denote by B(T) the σ-algebra of Borel sets for a given topological space T, typically\neither Sm−1 or G(m, m− 1). Let ϕ′ be a probability measure over B(Sm−1) that is even and\nassigns measure zero to each ( m − 2)-dimensional great sub-sphere of Sm−1. Then let x1, x2, ...,xn\nbe random points on Sm−1 distributed according to ϕ′. The corresponding random hyperplanes,\n{y ∈ Rm | xi · y = 0}i∈[n], will be in general position with probability 1. We will denote these\nrandom hyperplanes as H1, H2, ...,Hn, and the induced probability measure over these hyperplanes,\nwe will denote ϕ.\nDefinition 7 (Random Schl¨ afli Cones). Let ϕ be a probability measure as indicated above. Let n ∈ N",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_128",
    "file_path": "CSE_1.pdf"
  },
  "chunk-833288867385fec5f2d8fbeba95c8843": {
    "tokens": 341,
    "content": "+ ϵxj′\nis a strict improvement for a sufficiently small ϵ, given that the indifference curves\nand boundaries of the simplex are in general position and µ has full support.\nThen it suffices to show that the probability that there exists a vertex of the simplex such that\nnot all follower actions are a best response for at least one follower type is sufficiently small. By the\nsymmetry of Assumption 2 for a random leader strategy x, the probability that any given j ∈ [n] is\na best response for type θ is uniform over [n]. Therefore, define Eθ\ni,j to be the event where follower\naction j ∈ [n] is a best response to leader strategy xi for θ ∈ Θ and i ∈ [m]. Then, we must bound\nthe probability that for at least one i and j Eθ\ni,j does not happen for all θ ∈ Θ:\np = P\n\n [\ni∈[m]\n[\nj∈[n]\n\\\nθ∈Θ\n¬Eθ\ni,j\n\n ≤ mnP\n \\\nθ∈Θ\n¬Eθ\ni,j\n!\n= mn\n\u0012n − 1\nn\n\u0013|Θ|\n≤ mne−|Θ|\nn = O\n\u0010\ne−|Θ|\n\u0011\nSince O\n\u0010\n1\n|Θ|\n\u0011\n≥ O\n\u0000\ne−|Θ|\u0001\n, the probability that Assumption 1 is not satisfied is O\n\u0010\n1\n|Θ|\n\u0011\n.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_142",
    "file_path": "CSE_1.pdf"
  },
  "chunk-25da8d4f7f1a416014712a01b19e2d0c": {
    "tokens": 190,
    "content": "we show that this is not the case. In fact, we prove that the optimal dynamic policy is at least as\neffective as direct communication under a static policy, and we demonstrate scenarios in which the\nleader can learn a policy that achieves an Ω(1) improvement over the static policy.\nFinally, while our previously stated contributions use constructive techniques to demonstrate the\npositive results, they do not generally provide the optimal dynamic policy. As our final contribution,\nwe develop a mixed-integer linear program to compute the optimal dynamic policy. Unfortunately,\nit is straightforward to show that finding the optimal policy is NP-hard in general. Therefore, we\nalso develop two heuristics: one based on a Markov approach, and another that uses the first k\nrounds of the game to learn, followed by an exploitation phase. We show that these heuristics\nperform well on a set of standard Stackelberg games adapted for the repeated Bayesian setting.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_26",
    "file_path": "CSE_1.pdf"
  },
  "chunk-10b38c14502ee5b2bef7d5fa16f666c8": {
    "tokens": 374,
    "content": "|Θ|= 2 n= 5 n= 10n= 15m= 5 48/10051/10072/100m= 1027/10050/10062/100m= 1525/10048/10050/100\n|Θ|= 3 n= 5 n= 10n= 15m= 5 66/10083/10093/100m= 1048/10075/10087/100m= 1540/10070/10084/100\n|Θ|= 4 n= 5 n= 10n= 15m= 5 70/10090/100100/100m= 1062/10086/10093/100m= 1548/10086/10092/100\n|Θ|= 5 n= 5 n= 10n= 15m= 5 74/10094/10099/100m= 1059/10087/10096/100m= 1560/10087/10095/100\nTable 7: Probability of Assumption 1 Being Satisfied Under 100 Normal Random Bayesian\nStackelberg Game Instances. Probability of assumption 1 being satisfied under 100 random\nbayesian Stackelberg game instances. To generate each instance, we sample every game parameter\nRi,j and Cθ\ni,j ∀i ∈ [m], j∈ [n], θ∈ Θ independently from the standard normal distribution. The\nprior distribution µ is uniform over Θ. Each row represents the number of leader actions, and the\ncolumn represents the number of follower actions.\nincentive compatibility, the follower reports type θ truthfully and the leader plays a randomly",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_144",
    "file_path": "CSE_1.pdf"
  },
  "chunk-27abbb6db286c6d0f77d73771af9a3a8": {
    "tokens": 289,
    "content": "Efficiently: The Power of Randomization. In EC ’22: The 23rd ACM Conference on Economics\nand Computation. ACM, 705–735. https://doi.org/10.1145/3490486.3538270\n[16] Yanling Chang, Alan L Erera, and Chelsea C White. 2015. A leader–follower partially observed,\nmultiobjective Markov game. Annals of Operations Research 235, 1 (2015), 103–128.\n[17] Alon Cohen, Argyrios Deligkas, and Moran Koren. 2022. Learning Approximately Optimal Con-\ntracts. In Algorithmic Game Theory: 15th International Symposium, SAGT 2022, Colchester,\nUK, September 12–15, 2022, Proceedings. Springer, 331–346.\n[18] Vincent Conitzer and Tuomas Sandholm. 2006. Computing the optimal strategy to commit to.\nIn Proceedings of the 7th ACM conference on Electronic commerce. 82–90.\n[19] Pascal Courty and Li Hao. 2000. Sequential screening. The Review of Economic Studies 67, 4\n(2000), 697–717.\n[20] Quinlan Dawkins, Minbiao Han, and Haifeng Xu. 2021. The Limits of Optimal Pricing in the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_106",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b4a6f0a5f252acc25b95854e3ac1f70d": {
    "tokens": 245,
    "content": "(note the difference from table 2), while j0/j1 represents the follower rejects/accepts the price. For\nreadability, we relax the assumption that all utilities are in [0,1].\nExample 3 (Modified Pricing Game) . Define a modified pricing game, similar to Example 2,\nwhere the buyers valuations are again in the set V = {8, 35, 96} However, assume that the seller\ncan only offer the set of prices 22, 40, 61. Given these restricted prices, the seller/leader’s and the\nbuyer/follower’s utility matrices are defined as in Table 3.\nInterestingly, with this small modification to the pricing game, a dynamic policy indeed outper-\nforms the optimal static policy. The optimal static leader policy can be computed as\n\u00002\n3, 0, 1\n3\n\u0001\n, i.e.\nsetting an expected price of 35, which gives the leader an expected average utility of 231\n3. On the\nother hand, the optimal dynamic policy is to play a mixed strategy (0 , 0, 1) (i.e. set a price of 61) in",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_56",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c75522405d01eb7f37c25e31e91eb47b": {
    "tokens": 498,
    "content": "i\n\u0001\nNote that by Lemma 2 and given that k\n2 > m−2 we can define the function p(k) as an upper bound\non the above expectation as follows:\np(k) =\n\u0000k−1\nm−2\n\u0001 k−1−(m−3)\nk−1−(2m−5)\n\u0000k−1\nm−1\n\u0001\n+\n\u0000k−1\nm−2\n\u0001 k−1−(m−3)\nk−1−(2m−5)\n≥\nPm−2\ni=0\n\u0000k−1\ni\n\u0001\n\u0000k−1\nm−1\n\u0001\n+ Pm−2\ni=0\n\u0000k−1\ni\n\u0001 = E\n\"\nχ\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ H\n!#\n(25)\nIt is trivial to see that p(k) is decreasing in k for k >0. Therefore, for a k such that p(k) ≤ p, for\nany k ≥ k, p ≥ E\nh\n2U1,ϕ\n\u0010T\nθ∈Θ′ Hθ,+\ni,j\n\u0011i\n. For a given p we can solve for the k such that p(k) = p as\nfollows:\np\n\u0012k − 1\nm − 1\n\u0013\n= (1 − p)\n\u0012k − 1\nm − 2\n\u0013 k − 1 − (m − 3)\nk − 1 − (2m − 5).\nBy re-arranging and canceling:\n(k − m + 1)(k − 2m + 4) = 1 − p\np (m − 1)(k − m + 2).\nLet us denote q = 1−p\np . Then, we can rewrite this as a quadratic equation:\n0 = k2 + k(5 − 3m − q(m − 1)) + (2 +q)m2 − (6 + 3q)m + (4 + 2q)\nThis can be solved by the quadratic formula (choosing the positive root) as follows:\nk = 1\n2\n\u0012\n− ((q + 1)(1 − m) + 4− 2m) +\np",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_134",
    "file_path": "CSE_1.pdf"
  },
  "chunk-4f1616747d6d4be52dd6664e3eb43e65": {
    "tokens": 268,
    "content": "[49] Alessandro Pavan, Ilya Segal, and Juuso Toikka. 2014. Dynamic Mechanism Design: A\nMyersonian Approach. Econometrica 82, 2 (2014), 601–653. https://doi.org/10.3982/\nECTA10269\n[50] Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. 2019. Learning optimal strategies\nto commit to. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33.\n2149–2156.\n[51] Aaron Roth, Jonathan Ullman, and Zhiwei Steven Wu. 2016. Watch and learn: Optimizing\nfrom revealed preferences feedback. In Proceedings of the forty-eighth annual ACM symposium\non Theory of Computing . 949–962.\n[52] Rolf Schneider and Wolfgang Weil. 2008. Stochastic and Integral Geometry. Springer, Berlin,\nHeidelberg. https://doi.org/10.1007/978-[]3-[]540-[]78859-[]1\n[53] Lloyd S Shapley. 1953. Stochastic games. Proceedings of the national academy of sciences 39,\n10 (1953), 1095–1100.\n24",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_114",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5031b65e369581b1db120d2349562a49": {
    "tokens": 192,
    "content": "follower’s response to a particular leader’s strategy. Notably, these policies can learn the optimal\nleader strategy efficiently with a polynomial number of learning rounds, but these papers all assume\nthat the follower behaves myopically, i.e. the follower always best responds to the posted leader\nstrategy without regard for future rounds. Therefore, they are not characterizing the possiblity of\nlearning in strategic settings.\nRecently, Haghtalab et al. [31] studied learning in Stackelberg games with non-myopic agents and\nproposed a no-regret learning policy for the leader. However, their work relies on the assumption\nthat the follower discounts the future utility at a greater rate than the leader, a weakening of the\nstrategic assumptions on the follower. In our work, we assume symmetric discounting. In a slightly\ndifferent direction, Deng et al . [22] studies dynamic policy design in a repeated setting when the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_34",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ecf24312ddd473d46bb226090c4d6ffb": {
    "tokens": 180,
    "content": "variables, xt\njt−1, which means that Program (2) cannot be solved by standard solvers. However, we\ncan construct a MILP (see MILP (45) in Appendix D.1) that is solvable.\nTheorem 5. Program (2) and MILP (45) have the same optimal objective value and, moreover, an\noptimal solution of either can be efficiently recovered from an optimal solution of the other.\nWe defer the proof details to Appendix D.1. Though MILP (45) has nT|Θ| binary integer variables\ny, its running time does not depend on n exponentially. This is because any row of yθ (i.e. yθ\nt,·)\nhas only a single non-zero entry, thus there are in total O(nT|Θ|) many feasible y. Given a feasible\n17",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_89",
    "file_path": "CSE_1.pdf"
  },
  "chunk-81da8a985c18465b26e0ab77b68147fd": {
    "tokens": 281,
    "content": "a mixed strategy x ∈ ∆m, where ∆m = {x : P\ni∈[m] xi = 1 and 0 ≤ xi ≤ 1} is the m-dimensional\nsimplex and each xi denotes the probability the leader plays action i. After observing the leader\nstrategy x, the follower responds by playing some action j which leads to expected follower utility\nV (x, j) = P\ni∈[m] Ci,jxi. A rational follower will pick the optimal action j∗(x) = arg maxj∈[n] V (x, j)\nto maximize his own utility.2 The leader’s utility U(x, j) is defined similarly. When the follower’s\nutility matrix C is known to the leader, the leader can predict the follower’s reaction j∗(x) to any\nx. The rational leader will commit to x∗ = arg maxx∈∆m U(x, j∗(x)). This bi-level optimization\nproblem can be solved in poly(m, n) time via linear programming [ 18], and the optimal x∗ is the\nStrong Stackelberg Equilibrium (SSE).\n2.1.1 Bayesian Stackelberg Games\nIn many settings of interest, the leader may not know the follower’s payoff matrix C. To capture",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_41",
    "file_path": "CSE_1.pdf"
  },
  "chunk-62ff0f81fe6184fc4a7ac18570e6ac73": {
    "tokens": 236,
    "content": "respect to the ω0ωaCDM model.\nThe differences between the χ2 values of the DESI BAO DR1 and DR2 are presented in Table 4 and\ndemonstrate that the fit of all models improved. However, the improvement of the fit for the ΛCDM model is\nnon-significant, whereas the new measurements have a higher impact on the time-dependent dark energy models.\nIn the case of the Pantheon+ data, the improvement of the ΛCDM model is larger than for the DESY5 data.\nThe most significant change can be found for the ωCDM and νCKN models, for the DESY5 and Pantheon+\nsupernova dataset, respectively.\nThe improved statistics of the new measurements on the fit can also be seen in the correlation plots of the\nCKN and νCKN models for both supernova datasets in Figures 1 to 4, leading to a smaller 95 % and 68 %\nconfidence level (CL) area compared to the results from DESI BAO DR1.\n3 Discussion\nAs expected, the higher statistics coming with the DR2 leads to smaller uncertainties on the model parameters",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_5",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-60f02b3438daf54e502cd816536ac09d": {
    "tokens": 490,
    "content": "We are now ready to prove the main technical result, Theorem 3.\nProof. Proof of Theorem 3. Note that by Boole’s Inequality\nP\n\n [\ni∈[n]\n \\\nθ∈Θ\nBRθ(i)\n!\n̸= ∅\n\n ≤\nnX\ni=1\nP\n \\\nθ∈Θ\nBRθ(i) ̸= ∅\n!\n.\nLet us consider then the probability P\n\u0000T\nθ∈Θ BRθ(i) = ∅\n\u0001\n. It is easy to see that T\nθ∈Θ BRθ(i) =T\nθ∈Θ\nj∈[n]\\{i}\nHθ,+\ni,j ∩∆m. Let Θ ′ ⊆ Θ and let j be such that the distribution ϕ of Hθ′\ni,j is even and assigns\na measure zero to linear subspaces of Rm. Then, T\nθ∈Θ BRθ(i) ⊆ T\nθ′,∈Θ′ Hθ′,+\ni,j . Moreover, given\nAssumption 2 and Corollary 6, the expected probability that another random plane, H, distributed\naccording to ϕ will intersect T\nθ′∈Θ′ Hθ′,+\ni,j is given by:\nE\n\"\nχ\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ H\n!#\n=\nPm−2\ni=0\n\u0000|Θ′|−1\ni\n\u0001\nPm−1\ni=0\n\u0000|Θ′|−1\ni\n\u0001\nFor notational simplicity, we will denote k = |Θ′|. Then, simplifying\nE\n\"\nχ\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ H\n!#\n=\nPm−2\ni=0\n\u0000k−1\ni\n\u0001\n\u0000k−1\nm−1\n\u0001\n+ Pm−2\ni=0\n\u0000k−1\ni\n\u0001\nNote that by Lemma 2 and given that k\n2 > m−2 we can define the function p(k) as an upper bound",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_133",
    "file_path": "CSE_1.pdf"
  },
  "chunk-477f20e9a0f8550d219156df40ea3659": {
    "tokens": 166,
    "content": "leader, and we show that this condition holds with high probability. Despite the condition being\nmerely sufficient, we consider it an important contribution for characterizing the types of games\nwhere learning is effective.\nWhile we demonstrate that learning is generally effective in arbitrary repeated Bayesian Stackel-\nberg games, our setting differs from the broader mechanism design literature in that a Bayesian\nStackelberg game does not allow for arbitrary communication between the leader and follower. One\npotential interpretation of our first result might be that effective learning is simply a substitute for\ndirect communication. If so, and if the follower could directly communicate their type, the leader\nmight achieve a strict improvement over the optimal dynamic policy. As our second contribution,\nwe show that this is not the case. In fact, we prove that the optimal dynamic policy is at least as",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_25",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e649eea04db94e3f513a54507e24f4e4": {
    "tokens": 196,
    "content": "for the principal to redesign the contract given the revealed information from previous contracting\nperiods. In the context of security, whether it be cyber security, border patrol, airport security,\nprotecting wildlife from poachers, or many other security contexts, the defender, the leader, commits\nto a security policy and the attacker, the follower, responds [ 47, 63]. However, the value of different\ntargets to the attacker may be unknown, but through repeated rounds of defense and attack, the\nvalue may be learned based on the actions of the attacker. Tolling, a pricing based congestion\nmanagement approach, is also consistent with this setting. The central planner, the leader, sets\ntolls along road segments under which the traffic flow, the follower, best responds to both minimize\nlatency plus cost between any two nodes [ 51]. Here again, the central planner may learn about the\ndistribution of traffic over time through observing latency throughout the network.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_21",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b6396b98efd8fe6e8a616b1a3f81c210": {
    "tokens": 113,
    "content": "jθ\nt−1\n, jθ\nt ), which shows\nthe equivalence between the objective values of these two programs.\nD.1.2 Connection Between the MIP (47) and MILP (45)\nOur second step is to show the connection between the previous MIP (47) and our MILP (45). First\nof all, we present how to linearize the product term xt\njt−1,i\nQt\nt′=1 yθ\nt′,jt′ , we introduce two sets of\n41",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_165",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ac8d3975a78e1bb588d4389b75a5f7c3": {
    "tokens": 264,
    "content": "y, the program will be an LP with the size of variables bounded by a polynomial in m and n (given\nconstants T, |Θ|). This argument leads to the following corollary of Theorem 5.\nCorollary 2. The exact DSE can be computed by a MILP with O(T|Θ|mnT ) continuous variables\nand T|Θ|n integer variables; and it can be computed in poly(m, n) time with constants T and |Θ|.\n5.1 Computing the DSE Approximately\nIn this subsection, we present two approximation algorithms for approximating the DSE. The first\nMarkovian approach is a Markovian leader policy, where leader strategy xt only depends on the\ntime step t and the follower response jt−1. We can denote the markovian leader policy in round t\ngiven jt−1 is xt\njt−1 = π(jt−1) : [n] × [T] → ∆m.5 The specific MILP can be found in Appendix D.2.\nCorollary 3. The optimal dynamic Markovian policy can be computed by a MILP with a\nO(T|Θ|mn2) number of continuous variables and T|Θ|n integer variables.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_90",
    "file_path": "CSE_1.pdf"
  },
  "chunk-1c02c209db7215254ae94050098d2835": {
    "tokens": 239,
    "content": "Step 2: Constructing an approximately-optimal strictly-IC randomized menu. There\nare two reasons why we need strict incentive compatibility when simulating the randomized menu.\nThe first is to incentivize every follower θ to respond with H(θ) in the above Step 1. To achieve this,\nwe need to guarantee that, from θ’s point of view, the strategy sequence in the remaining T − t\nrounds for him is strictly better — in fact at least additively t better — than the strategy sequence\nfor any other θ′. The second reason is more intrinsic: we have to round all the probabilities in the\nrandomized menu being played after t into multipliers of 1/(T − t), such that they can be precisely\nrealized by deterministic strategies. Because of this, we also need a strictly IC menu to make up\nfor the incentive distortion during the rounding of probabilities. In this step, we show that there\nalways exists such a randomized menu that is O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\noptimal and O\n\u0012q\nlog |Θ|\nT\n\u0013\n-strictly IC\n35",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_147",
    "file_path": "CSE_1.pdf"
  },
  "chunk-49b2b49e4cb47d3ffc3bfa9291228c17": {
    "tokens": 328,
    "content": "\u0000\nnT \u0001\nand the exponential-in-|Θ| time is due to the NP-hardness even\nwhen T = 1. However, deriving such a MILP is nontrivial since a naive formulation of the problem\nis non-linear. The key technical challenge is deriving a MILP and demonstrating that, though the\nprogram is not strictly equivalent to a program that computes the DSE, any solution of the MILP\ncan be efficiently converted to a solution for the program that defines the DSE.\nThe program that maximizes the leader’s total utility subject to the constraint that jθ\nT is the\noptimal response sequence for follower type θ is as follows:\nmaximize\nX\nt∈[T]\nX\nθ∈Θ\nh\nµ(θ) U(xt\njθ\nt−1\n, jθ\nt )\ni\n(2a)\nsubject to (2b)\nX\nt∈[T]\nV θ(xt\njθ\nt−1\n, jθ\nt ) ≥\nX\nt∈[T]\nV θ(xt\nbjt−1\n,bjt), ∀θ ∈ Θ, bjT ∈ [n]T , jθ\nT ∈ [n]T , ∀θ ∈ Θ. (2c)\nNote that in Program (2), the decision variables jθ\nt−1 appear in the indices of other decision\nvariables, xt\njt−1, which means that Program (2) cannot be solved by standard solvers. However, we",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_88",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9203090542c12e68902912fcdd3175df": {
    "tokens": 477,
    "content": "Table 2: Results of the best-fit points from the ΛCDM, ωCDM and ω0ωaCDM model to the datasets of DESI\nBAO DR2 and Hubble, once with DESY5 and once with Pantheon+ data. Shown are the results for Hubble-\ntoday H0, the matter density parameter Ω 0\nM, the drag epoch rd, the parameters ω0 and ωa, and the minimal\nχ2\nmin/DOF.\nModel H0 in Ω 0\nM rd in ω or ω0 ωa χ2\nmin/DOF\n/Datasets km/s/Mpc Mpc\nΛCDM\n+ DESY5 69 .77 ± 2.38 0 .309 ± 0.008 144 .28 ± 4.85 – – 1681/1871\n+ Pantheon+ 70 .10 ± 2.39 0 .303 ± 0.008 144 .21 ± 4.85 – – 1439/1632\nωCDM\n+ DESY5 68 .71 ± 2.37 0 .297 ± 0.009 144 .11 ± 4.85 −0.88 ± 0.04 – 1670/1870\n+ Pantheon+ 69 .32 ± 2.40 0 .297 ± 0.008 144 .10 ± 4.85 −0.92 ± 0.04 – 1435/1631\nω0ωaCDM\n+ DESY5 68 .69 ± 2.50 0 .321 ± 0.013 143 .70 ± 5.01 −0.78 ± 0.07 −0.77 ± 0.39 1668/1869\n+ Pantheon+ 69 .49 ± 2.28 0 .302 ± 0.017 143 .60 ± 4.58 −0.91 ± 0.06 −0.11 ± 0.43 1434/1630\nTable 3: Comparison between CKN and νCKN and alternative cosmological models. Shown is the difference\n∆χ2 = χ2,(ν)CKN",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_7",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-09b4f5c1a705eed05c0dde72c4f94556": {
    "tokens": 447,
    "content": "Note that m ≥ 2 implies −4m + 8 ≤ 0. Therefore if we choose k such that\nk ≥ 1\n2\n\u0012\n((q + 1)(m + 1) − 4 + 2m) +\np\n(m − 1)2(q + 1)2\n\u0013\n≥ 1\n2\n\u0012\n((q + 1)(m + 1) − 4 + 2m) +\np\n(m − 1)2(q + 1)2 − 4m + 8\n\u0013\nTherefore, for q = 1−p\np set k = m(q + 2) = m\n\u0010\n1\np + 1\n\u0011\n, then for all k ≥ k:\np ≥ p(k) ≥ E\n\"\nχ\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ H\n!#\n. (26)\nEquation 26 is a bound on the expected probability that a random hyperplane from G(m, m− 1)\ndrawn according to ϕ intersects the set T\nθ∈Θ′ Hθ,+\ni,j . The probability can be bounded directly by\nusing the Markov inequality:\nP\n \nχ\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ H\n!\n> dp\n!\n≤ 1\nd. (27)\nNow consider the ( |Θ| − |Θ′|)-hyperplanes generated by the set of types Θ \\ Θ′. Consider a\nrandom hyperplane in this set Hθ\ni,j for some θ ∈ Θ \\ Θ′. With probability greater than 1 − 1\nd, the\nchance that Hθ\ni,j intersects T\nθ′∈Θ′ Hθ′,+\ni,j is less than dp. If Hθ\ni,j does not intersect T\nθ′∈Θ′ Hθ′,+\ni,j , then\nthere is a 1\n2 chance that T\nθ′∈K Hθ′,+\ni,j ∩ Hθ,+",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_136",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5a5d8367b1e734b6ccfcde929205f2de": {
    "tokens": 163,
    "content": "i,j∗θ\nt\n, ∀θ. (49)\nThat is, aθ is precisely follower type θ’s total utility from response yθ.\nNow let us consider any other possible response sequence jT ∈ [n]T ̸= j∗θ\nT . Since yθ\nt is a one-hot\nvector and yθ\nt,jt = 1 if and only if jt = j∗θ\nt . Therefore, if jT ̸= j∗θ\nT , there must exists some t such\nthat yθ\nt,jt = 0, hence, X\nt\nyθ\nt,jt < T, ∀θ, jT ̸= j∗θ\nT .\n40",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_161",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e40f734d753aaaedc9580e3a4d1de705": {
    "tokens": 232,
    "content": "of the (ν)CKN models with other dark energy models in the literature, and discuss the difference to the fit for\nwhich the DESI BAO Year-1 data release (DR1) [8] is used instead. Finally, we summarize and discuss the\nresults in Section 3.\n2 Updated Results\nAnalogously to the statistical procedure described in Reference [1], we perform a χ2 fit and provide the resulting\nbest-fit points of the CKN and νCKN model in Table 1. Since we use two different supernova datasets, DES-\nSN5YR (DESY5) and Pantheon+, we always combine the DESI BAO DR2 and Hubble data with one of them\n∗patrick.adolf@tu-dortmund.de\n†mahirsch@ific.uv.es\n‡sara.krieg@tu-dortmund.de\n§heinrich.paes@tu-dortmund.de\n¶mustafa.tabet@tu-dortmund.de\n1\narXiv:2504.15332v1  [astro-ph.CO]  21 Apr 2025",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_2",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-20019031fcf1fd22a7374897255a98c4": {
    "tokens": 315,
    "content": "across all follower types. Again, this seems a natural condition, given that if a single response is\nlikely to always be a best response, then the leader cannot benefit from learning the follower’s type.\nAssumption 2 (Generic Conditions for Follower Payoffs). Consider the random Bayesian Stackelberg\nGame {R, Θ, {Cθ}θ∈Θ, µ, f}. For each j ∈ [n], there exists a j′ ∈ [n] such that the distribution,\ndenoted ϕ′, of (Cθ\n·,j − Cθ\n·,j′) ∈ Rm induced by f, is even and assigns measure zero to a linear subspace\nof Rm. I.e., for X ⊂ Rm and −X = {x : −x ∈ X} ⊂Rm, ϕ′(X) = ϕ′(−X), and for any linear\nsubspace L ⊂ Rm, ϕ′(L) = 0.\nNote that ϕ′ in Assumption 2 induces a distribution, which we will denote ϕ, over Hθ\ni,j and\nHθ,+\ni,j . The technical result that Assumption 2 provides is that it ensures that for all j ∈ [n]\nthe random hyperplanes {Hθ\nj,j′}θ∈Θ are in general position with probability 1 [ 35]. A set of k-",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_70",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5afb3ada08f41766223b89b76c18f025": {
    "tokens": 610,
    "content": "i,j does not intersect T\nθ′∈Θ′ Hθ′,+\ni,j , then\nthere is a 1\n2 chance that T\nθ′∈K Hθ′,+\ni,j ∩ Hθ,+\ni,j = ∅, by the assumption that ϕ is even.\nDenote by J the condition such that χ\n\u0010T\nθ′∈Θ′ Hθ′,+\ni,j ∩ H\n\u0011\n≤ dp\nP\n \\\nθ∈Θ\nBRθ(i) ̸= ∅\n!\n≤ P\n\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩\n\\\nθ∈Θ\\Θ\nHθ,+\ni,j ̸= ∅\n\n (28)\n= P\n\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩\n\\\nθ∈Θ\\Θ\nHi, jθ,+ ̸= ∅ |J\n\nP (J) + P\n\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩\n\\\nθ∈Θ\\Θ\nHi, jθ,+ ̸= ∅ | ¬J\n\nP (¬J)\n(29)\n≤ P\n\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩\n\\\nθ∈Θ\\Θ\nHθ,+\ni,j ̸= ∅ |J\n\n + 1\nd (30)\n= P\n\n \\\nθ∈Θ\\Θ′\n\\\nθ∈Θ′\nHθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅ |J\n\n + 1\nd (31)\n≤ P\n\n \\\nθ∈Θ\\Θ′\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅\n!\n| J\n\n + 1\nd (32)\n=\nY\nθ∈Θ\\Θ′\nP\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅ |J\n!\n+ 1\nd. (33)\nNote that equation 29 is due to the law of total probability, equation 32 is true since if the member\nof an intersection of sets is empty then the intersection will also be empty, and equation 33 is\n32",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_137",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9d152dc35a6bd84c4bcd85f738ba2a9a": {
    "tokens": 253,
    "content": "[30] Gurobi Optimization, LLC. 2022. Gurobi Optimizer Reference Manual. https://www.gurobi.\ncom\n[31] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. 2022. Learning in\nStackelberg Games with Non-myopic Agents. In Proceedings of the 23rd ACM Conference on\nEconomics and Computation . 917–918.\n[32] Nika Haghtalab, Chara Podimata, and Kunhe Yang. 2024. Calibrated stackelberg games:\nLearning optimal commitments against calibrated agents. Advances in Neural Information\nProcessing Systems 36 (2024).\n[33] Minbiao Han, Michael Albert, and Haifeng Xu. 2024. Learning in online principal-agent\ninteractions: The power of menus. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 38. 17426–17434.\n[34] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. 2014. Adaptive contract\ndesign for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_109",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ce56734b2704a2f87089b83709986060": {
    "tokens": 383,
    "content": "where M is a very large constant, a ∈ Rk is a set of newly introduced decision variables.\nTo see how Inequality (48) characterizes the optimality condition of the follower’s best responses,\nconsider any variables x, y and a feasible to Inequality (48). Recall that yθ\nt ∈ {0, 1}n is a one-hot\nvector of length n for any θ ∈ Θ and t ∈ [T]. We denote j∗θ\nt ∈ [n] as the index of the unique 1-valued\nentry in yθ\nt , and vector j∗θ\nT = (j∗θ\n1 , ··· , j∗θ\nT ). We argue that if x, y and a satisfy Inequality (48),\nthen j∗θ\nT must be an optimal response sequence for type θ.\nBy construction, we have P\nt yθ\nt,j∗θ\nt\n= T, for all θ and j∗θ\nT by definition. Plugging this into\nInequality (48), we obtain M(T − P\nt yθ\nt,j∗θ\nt\n) = 0. Thus Inequality (48) becomes 0 ≤ (aθ −\nP\nt∈[T]\nP\ni∈[m] xt\nj∗θ\nt−1,iCθ\ni,j∗θ\nt\n) ≤ 0, which implies\naθ =\nX\nt∈[T]\nX\ni∈[m]\nxt\nj∗θ\nt−1,iCθ\ni,j∗θ\nt\n, ∀θ. (49)\nThat is, aθ is precisely follower type θ’s total utility from response yθ.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_160",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ce1f803d40b51344f1964a38ad57abdb": {
    "tokens": 169,
    "content": "0.32 0.34 0.36 0.38\n60\n65\n70\n75\n0.32 0.34 0.36 0.38\n130\n135\n140\n145\n150\n155\n160\n60 65 70 75\n130\n135\n140\n145\n150\n155\n160\nFigure 1: Correlations of H0–Ω0\nM (top left), H0–rd (top right), rd–Ω0\nM (bottom left) in the CKN model for\nthe DESI BAO+Hubble+DESY5 DR1 (black dashed lines) and DESI BAO+Hubble+DESY5 DR2 (blue area)\ndataset at the 95 % and 68 % CL.\n4",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_9",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-8eb366236e818034a9fb9fadbe6995b3": {
    "tokens": 195,
    "content": "and Markovian policy is significant (note the runtime of DSE when T = 4 is already much higher\nthan the Markovian method when T = 5), so we only test the first 10 random instances out\nof the 50 random instances, which explains why the RME performs differently when T ≥ 4. In\naddition, the runtime of Markovian when T = 6 is already much higher than the First-k method\nwhile achieving a lower utility. The advantage of DSE over RME on the average leader utility is\nalso consistent with our observation in previous structured games. Finally, we note that First-k\npolicies achieve near-optimal utility with significantly improved computational efficiency compared\nto computing the optimal dynamic policy. In appendix E, we report some additional experimental\nresults on both structured and randomized games.\n7 Conclusion\nIn this paper, we have shown that, contrary to the intuition suggested by the No Learning Theorem",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_99",
    "file_path": "CSE_1.pdf"
  },
  "chunk-89a8811ce74d2e4719206c4e74596baa": {
    "tokens": 208,
    "content": "Our setting, if viewed as a restricted dynamic mechanism design problem, is also related to Pavan\net al. [48]. The No Learning Theorem can be seen as a direct consequence of the characterization\nof the optimal mechanism given by Pavan et al . [48], although their work does not address the\nlikelihood that learning is generally effective. Similar results to the No Learning Theorem have been\nindependently identified in the literature, going back to at least Baron and Besanko [9]. Baron and\nBesanko [9] studied a two-period interaction between a regulator and a regulated firm, where the\nagent’s type in the second round evolves from the first round. They show that the multi-period\nproblem reduces to a static problem when the agent type remains the same across periods, consistent\nwith the insights from the No Learning Theorem. Other work, such as Courty and Hao [19], has\nstudied the optimal mechanism for a two-period ticket-selling problem. In contrast to our work,",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_30",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5ff3d77f2e21dafac8d65a724b150a80": {
    "tokens": 144,
    "content": "In this paper, we have shown that, contrary to the intuition suggested by the No Learning Theorem\nfor dynamic pricing, it is generally feasible for the leader to learn effectively in fully strategic settings.\nSpecifically, the leader can improve her utility through information acquisition in repeated play\nagainst a strategic opponent, and this learning is not merely a substitute for communication.\nWe have also developed a novel mixed-integer linear program to solve for the optimal DSE, albeit\nwith exponential time and space complexity. To address this computational challenge, we provided\ntwo heuristics that perform well and significantly improve computational scalability, though they\nremain exponential in the number of follower types given that the BSE is NP-hard to compute.\n20",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_100",
    "file_path": "CSE_1.pdf"
  },
  "chunk-db8eabb4aca6f6890f7385bae9c89dcc": {
    "tokens": 311,
    "content": "optimal follower response path for follower type θ. This guarantees that Objective (51) only counts\nthe jt on the optimal path j∗θ\nT . Therefore, it correctly calculates the expected leader utility when\neach follower type θ follows their optimal response paths.\nShowing the equivalence. Consider x and j as a feasible solution of (2). We will show that\nx, y θ\nt,j =\n(\n1 if j = jθ\nt\n0 otherwise, and aθ =\nX\nt∈[T]\nuf\nθ (xt\njθ\nt−1\n, jθ\nt )\nis a feasible solution of (47) of the same objective function value. The last four constraints of (47) are\nsatisfied by construction. To see how the first constraint is satisfied, note for any jT ̸= jθ\nT returned\nby (2) of type θ, we have P\nt∈[T] yθ\nt,jt < T by construction, and its corresponding follower utilityP\nt,i xt\njt−1,iCθ\ni,jt ≤ aθ by the definition that jT is not the best response. When jT = jθ\nT , we have\nM(T −P\nt∈[T] yθ\nt,jt) = 0 and aθ = P\nt,i xt\njt−1,iCθ\ni,jt by construction. Hence, the first constraint of (47)",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_163",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5699ca58feaf6260b0e38b975c96aa6a": {
    "tokens": 202,
    "content": "It may seem a-priori reasonable that a strategic leader should be able to induce a subgroup with\ndistinct best responses to eventually reveal their type. However, it is not obvious that this could be\ndone in such a manner that the leader is made better off versus the optimal static strategy, which\nwe will show is indeed the case. More importantly, given the precise condition of Assumption 1\nthat ensures that effective learning is possible, we can leverage the assumption to show that for\nrandom games, according to a certain definition of random, learning is very likely to be effective, as\nwe will demonstrate in Section 3.2. This strongly suggests that we should not expect a No Learning\nTheorem type result to hold for an arbitrary class of Bayesian Stackelberg games. Instead, insights\nfrom the No Learning Theorem should be viewed as narrowly applicable to the dynamic pricing\ngame.\nFinally, Assumption 1 is a sufficient condition, not a necessary condition. For example, the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_64",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c46b723b3caf5b0b219ef5af51c55059": {
    "tokens": 201,
    "content": "incentive compatibility, the follower reports type θ truthfully and the leader plays a randomly\nselected strategy. However, the dynamic policy plays a single strategy at each round. As a result,\nthe dynamic policy must use the initial rounds to learn the follower’s type θ and then simulate\nthe corresponding randomized strategy on the RME. Another challenge comes from the simulation\nof randomization over strategies with deterministically chosen strategies. An intuitive idea is to\nplay x∗\nθ,j for the number of rounds that is proportional to p∗\nθ,j. Unfortunately, this requirement\ncannot be exactly fulfilled unless the number of rounds can be divided evenly for every j according\nto p∗\nθ,j; Otherwise, the follower’s incentives may be distorted. An overview of the high-level idea is\nrepresented in the Figure 4. We show how to address these challenges in three steps.\nFigure 4: Illustration of the Sequence of Leader Strategies. The leader first plays a strategy for",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_145",
    "file_path": "CSE_1.pdf"
  },
  "chunk-db445fc840adf54e91ee634e1bccf154": {
    "tokens": 233,
    "content": "the first round. If the price is rejected, the leader switches to the mixed strategy of\n\u00002\n3, 0, 1\n3\n\u0001\n(i.e. set\na price of 35); the leader keeps the same price of 61 if the price is accepted in the first round. As a\nresult, Follower C0 with private value 8 rejects for two rounds; Follower C2 with private value 96\naccepts the price for two rounds; Follower C1 rejects the price in the first round and then accepts in\nthe second round. This dynamic policy gives the leader an expected average utility of 261\n6, showing\nthat the optimal dynamic policy outperforms the optimal static policy.\nThis example also provides some intuition about one question an attentive reader may ask,\n“Given that the allocation rule gives the seller more power than the leader in a general Stackelberg\ngame, why is the seller less able to learn than in the general Stackelberg game?” Notably, in the\nabove example, the optimal dynamic utility is still worse than Myerson’s revenue of 32 when the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_57",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e73dd022f3f103680749efac326aa3f2": {
    "tokens": 215,
    "content": "O(T|Θ|mn2) number of continuous variables and T|Θ|n integer variables.\nComparing the DSE, our Markovian policy reduces the exponential number of continuous\nvariables to a polynomial size. However, despite this improvement, the runtime of the Markovian\npolicy still exhibits exponential growth relative to T, making it suboptimal for addressing dynamic\nleader-follower interactions within a large T. Therefore, we introduce the following second heuristic\napproach, First-k, a simplified variant of DSE where the leader’s dynamic policy only depends on\nthe initial k-rounds of interactions. Following the initial k-rounds, the leader plays a static strategy\nfor the remaining T − k rounds. We defer the specific MILP to Appendix D.2.\nCorollary 4. The optimal dynamic First-k policy can be computed by a MILP with a O\n\u0000bk|Θ|mnbk\u0001\nnumber of continuous variables and |Θ|nbk integer variables, where bk = min(k + 1, T).",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_91",
    "file_path": "CSE_1.pdf"
  },
  "chunk-1a164080268730891d700dde69160396": {
    "tokens": 395,
    "content": "maximize\nX\nt∈[T]\nX\nθ∈Θ\nh\nµ(θ) U(xt\njθ\nt−1\n, jθ\nt )\ni\n(53a)\nsubject to (53b)\nX\nt∈[T]\nV θ(xt\njθ\nt−1\n, jθ\nt ) ≥\nX\nt∈[T]\nV θ(xt\nbjt−1\n,bjt), ∀θ ∈ Θ, bjT ∈ [n]T , jθ\nT ∈ [n]T , ∀θ ∈ Θ. (53c)\nD.2.2 MILP to compute the First-k policy.\nmaximize\nX\nθ∈Θ\nµ(θ)\n\" X\nt∈[k]\nU(xt\njθ\nt−1\n, jθ\nt ) + U(xk+1\njθ\nk\n, jθ\nk+1) · (T − k)\n#\n(54a)\nsubject to (54b)\nX\nt∈[k]\nV θ(xt\njθ\nt−1\n, jθ\nt ) + V θ(xk+1\njθ\nk\n, jθ\nk+1) · (T − k) ≥\nX\nt∈[k]\nV θ(xt\nbjt−1\n,bjt) + V θ(xk+1\nbjk\n,bjk+1) · (T − k),\n∀θ ∈ Θ, bjk+1 ∈ [n]k+1, jθ\nk+1 ∈ [n]k+1, ∀θ ∈ Θ. (54c)\nE Additional Experimental Results\nIn this section, we provide some additional experimental results not included in Section 6.\n43",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_172",
    "file_path": "CSE_1.pdf"
  },
  "chunk-86037c87c1c6719a5a6ac9b1ba7f557a": {
    "tokens": 200,
    "content": "i,j .\nDefinition 4 (Random Bayesian Stackelberg Game). A random Bayesian Stackelberg Game is a\nStackelberg game with leader utility matrix R and prior distribution µ over a fixed set of types Θ\nwhere the follower utility matrices, {Cθ}θ∈Θ, are independently and identically distributed according\nto f. We denote a random game by {R, Θ, {Cθ}θ∈Θ, µ, f}.\nHowever, it is clear that an arbitrary distribution f will not provide any non-zero bound on\nthe probability that the random Bayesian Stackelberg game permits effective learning since any\nprobability distribution that strictly generates pricing games does not permit effective learning with\nprobability 1. Therefore, we must put some conditions on the generating distribution in order to\nensure that the resulting game is likely to permit effective learning. Our condition is fundamentally\nthat no particular follower action is more likely to be the best response for a given leader strategy",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_69",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b796c28e23e34488edad2c65d32d6ee3": {
    "tokens": 235,
    "content": "learns effectively for dynamic Bayesian Stackelberg games and that this learning is not merely\na substitute for communication. In this section, we develop algorithms to compute an effective\ndynamic policy. However, when T = 1, the DSE degenerates to the BSE, which is known to be\nNP-hard [18]. Moreover, with T >1, even writing down a dynamic policy takes space exponential in\nT as it has to specify a strategy for each possible sequence of follower actions. Thus, our focus is on\ndeveloping practical algorithms for computing the DSE for small games. Towards that end, the main\nresult of this section is a Mixed Integer Linear Program ( MILP) formulation for computing the DSE.\nThis MILP has O(nT m|Θ|) continuous variables and O(T|Θ|) integer variables and thus takes time\nexponential in T, |Θ| to solve. The exponential time dependence on T, |Θ| is expected since the size\nof the policy description is Ω\n\u0000\nnT \u0001\nand the exponential-in-|Θ| time is due to the NP-hardness even",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_87",
    "file_path": "CSE_1.pdf"
  },
  "chunk-09a3d930e26e1161ba35f6f20ac67c68": {
    "tokens": 315,
    "content": "is a convex body in Sm−1 that is not a great sub-sphere, χ(K ∩ L) = 1 (K ∩ L ̸= ∅) for almost all\nL ∈ G(m, m− j), see p. 40 of Schneider and Weil [52]. Therefore if ϕ is an absolutely continuous\nmeasure with respect to νm−j, 2Uj,ϕ(K) is the probability measure of a random ( m −j)-dimensional\nlinear subspace, sampled according to ϕ, intersecting with the set K.\nConsider the set of hyperplanes H1, H2, ..., Hn ∈ G(m, m−1). We will follow Hug and Schneider\n[35] and say that they are in general position if any k ≤ m of them have an intersection of dimension\nm − k.\nIf the hyperlanes H1, H2, ..., Hn ∈ G(m, m− 1) are in general position, they induce a coni-\ncal tessalation [ 35] of Rm into m-dimensional polyhedral cones, denoted by T . We will write\nF(H1, H2, ..., Hn) to denote the set of polyhedral cones in T . For C ∈ F(H1, H2, ..., Hn), the convex\nsets C ∩ Sm−1 form a spherical tessalation of Sm−1. If we denote arbitrarily by H− one of the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_126",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c5829aae6e99e7807ef9b69937b33a55": {
    "tokens": 289,
    "content": "design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In\nProceedings of the fifteenth ACM conference on Economics and computation . 359–376.\n[35] Daniel Hug and Rolf Schneider. 2016. Random Conical Tessellations. Discrete &\nComputational Geometry 56, 2 (Sept. 2016), 395–426. https://doi.org/10.1007/\ns00454-[]016-[]9788-[]0\n[36] Nicole Immorlica, Brendan Lucier, Emmanouil Pountourakis, and Samuel Taggart. 2017.\nRepeated Sales with Multiple Strategic Buyers. In Proceedings of the 2017 ACM Conference\non Economics and Computation (Cambridge, Massachusetts, USA) (EC ’17). Association for\nComputing Machinery, New York, NY, USA, 167–168. https://doi.org/10.1145/3033274.\n3085130\n[37] Sham M Kakade, Ilan Lobel, and Hamid Nazerzadeh. 2013. Optimal dynamic mechanism\ndesign and the virtual-pivot mechanism. Operations Research 61, 4 (2013), 837–854.\n[38] Jean-Jacques Laffont and Jean Tirole. 1988. The dynamics of incentive contracts. Econometrica:",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_110",
    "file_path": "CSE_1.pdf"
  },
  "chunk-30dccb151c622761364beb1f8ae13bc8": {
    "tokens": 324,
    "content": "B Omitted Details for Section 3\nIn this appendix, we provide the proofs of the results stated in Section 3.\nB.1 Proof Details for Section 3.1\nIn this section, we provide the proof of Theorem 2.\nProof. Proof of Theorem 2. We prove this by construction. Specifically, we construct a dynamic\nleader policy that achieves higher leader utility than the BSE strategy. We do this by constructing\na policy that is a local deviation from the BSE. We denote the BSE leader strategy as x∗ for\n{R, Θ, {Cθ}θ∈Θ, µ}. By assumption 1, there exists a sub-group Θ′ ⊂ Θ such thatBR(Θ′)∩BR(Θ\\Θ′) =\n∅ and BSE(Θ′) ̸= x∗. Denote BSE(Θ′) as bx. We consider the dynamic leader policy, π, defined as\nfollows:\nx1 = ··· = xT−1 = x∗; xT =\n(\nbx if j1, ··· , jT−1 ∈ BR(Θ′);\nx∗ otherwise.\nTo ensure that all types in Θ ′ respond with action in BR(Θ′) for rounds 1 , ..., T− 1 and all types in\nΘ \\ Θ′ do not respond with action in BR(Θ′) for any round, we need the following set of constraints\nto be satisfied:\nX\nθ∈Θ′",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_121",
    "file_path": "CSE_1.pdf"
  },
  "chunk-4a503b3315b1c3724164c76ad287d746": {
    "tokens": 328,
    "content": "[2202.04077].\n[5] DES collaboration, T. M. C. Abbott et al., The Dark Energy Survey: Cosmology Results With ˜1500 New\nHigh-redshift Type Ia Supernovae Using The Full 5-year Dataset , 2401.02929.\n[6] A. Favale, M. G. Dainotti, A. G´ omez-Valent and M. Migliaccio,Towards a new model-independent calibra-\ntion of Gamma-Ray Bursts , 2402.13115.\n[7] M. Moresco, R. Jimenez, L. Verde, A. Cimatti and L. Pozzetti, Setting the Stage for Cosmic Chronometers.\nII. Impact of Stellar Population Synthesis Models Systematics and Full Covariance Matrix , Astrophys. J.\n898 (2020) 82, [2003.07362].\n[8] A. G. Adame et al. [DESI], “DESI 2024 VI: cosmological constraints from the measurements of baryon\nacoustic oscillations,” JCAP 02 (2025), 021 doi:10.1088/1475-7516/2025/02/021 [arXiv:2404.03002 [astro-\nph.CO]].\n[9] M. Chevallier and D. Polarski, Accelerating universes with scaling dark matter , Int. J. Mod. Phys. D 10\n(2001), 213-224, [gr-qc/0009008].",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_14",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-cef43c60aa637ee8f37aba5e07c87731": {
    "tokens": 395,
    "content": "Table 1: Results of the best-fit points for the CKN and νCKN case to the datasets of DESI BAO DR2 and\nHubble, once combined with DESY5 and once with Pantheon+ data. Shown are the results for Hubble-today\nH0, the matter density parameter Ω 0\nM, the drag epoch rd, the parameter ν and the minimal χ2\nmin over the\ndegrees of freedom (DOF).\nModel/Datasets H0/(km/s/Mpc) Ω 0\nM rd/Mpc ν χ 2\nmin/DOF\nCKN\n+ DESY5 68 .83 ± 2.35 0 .352 ± 0.009 144 .27 ± 4.85 – 1674/1871\n+ Pantheon+ 69 .09 ± 2.36 0 .347 ± 0.009 144 .23 ± 4.85 – 1437/1632\nνCKN\n+ DESY5 68 .90 ± 2.38 0 .348 ± 0.018 144 .26 ± 4.85 0 .92 ± 0.35 1674/1870\n+ Pantheon+ 69 .46 ± 2.40 0 .330 ± 0.018 144 .21 ± 4.85 0 .64 ± 0.36 1436/1631\nseparately. The resulting χ2 values show that both the CKN and the more general νCKN models are well\ncompatible with the experimental measurements, as the goodness of the best-fit points over the degrees of\nfreedom (DOF) are χ2/DOF ≈ 0.89 and χ2/DOF ≈ 0.88 for the DESY5 and Pantheon+ data, respectively.",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_3",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-14967f7b1dc86c31f5eb7adf3d3cd6ee": {
    "tokens": 134,
    "content": "over columns does not assign positive measure to a linear subspace of Rm. The second condition in\nAssumption 2 requires that any linear subspace of Rm is assigned measure zero by ϕ. However, in\nall the following results, this can be relaxed for cases under which the support of the distribution\nis confined to a linear subspace of dimension k ≤ m but otherwise assigns measure zero to linear\nsubspaces of dimension k′ ≤ k. This allows for settings under which the follower’s payoffs for two or\nmore leader actions are perfectly correlated. For the sake of exposition, we will assume that ϕ′ does\n13",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_72",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e61ced6156ea8d394ce84a692e0d8b90": {
    "tokens": 282,
    "content": "[44] Mehryar Mohri and Andres Munoz. 2015. Revenue optimization against strategic buyers. In\nAdvances in Neural Information Processing Systems . 2530–2538.\n[45] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations research 6, 1\n(1981), 58–73.\n[46] Roger B Myerson. 1982. Optimal coordination mechanisms in generalized principal–agent\nproblems. Journal of mathematical economics 10, 1 (1982), 67–81.\n[47] Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and\nSarit Kraus. 2008. Playing games for security: An efficient exact algorithm for solving Bayesian\nStackelberg games. In Proceedings of the 7th international joint conference on Autonomous\nagents and multiagent systems-Volume 2 . 895–902.\n[48] Alessandro Pavan, Ilya Segal, and Juuso Toikka. 2014. Dynamic mechanism design: A\nmyersonian approach. Econometrica 82, 2 (2014), 601–653.\n[49] Alessandro Pavan, Ilya Segal, and Juuso Toikka. 2014. Dynamic Mechanism Design: A",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_113",
    "file_path": "CSE_1.pdf"
  },
  "chunk-d2d68eb6dddc43ee77e80690418429ce": {
    "tokens": 338,
    "content": "and let H1, H2, ...,Hn ∈ G(m, m− 1) be independent random hyperplanes distributed according to\nϕ. The random (ϕ, n)-Schl¨ afli coneSn is the cone chosen uniformly at random from the polyhedral\ncones induced by H1, H2, ...,Hn.\nWith a slight abuse of notation, denote by Hn = H1, H2, ..., Hn. We follow Hug and Schneider\n[35] (see p. 406) and formally define Sn as the polyhedral cone with distribution given by:\nP(Sn ∈ B) =\nZ\nG(m,m−1)n\n1\nC(n, m) ·\nX\nC∈F(Hn)\n1 B(C)ϕn(d(Hn)) (14)\nNow, let L ∈ G(m, k) be in general position with respect to H1, H2, ..., Hn ∈ G(m, m−1). Then\nL ∩ H1, ..., L∩ Hn are (k − 1)-dimensional subspaces of L. Moreover, they are in general position in\nL. Therefore, there are C(n, k) k-dimensional Schl¨ afli Cones cones in the tessalationTL, i.e. the\ntessalation induced in L by H1, H2, ..., Hn.\nAgain, if we let H1, H2, ...,Hn ∈ G(m, m−1) be distributed according to ϕ, and let L ∈G(m, k)",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_129",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c0c1d20c7c2d160c386aab5a38a6b1c1": {
    "tokens": 389,
    "content": "∆χ2 = χ2,(ν)CKN\nmin − χ2,alt.model\nmin for both datasets and the difference ∆AIC. The negative values indicate a\npreference of CKN and νCKN over the alternative models, respectively. See text for a comparison of the νCKN\nand ΛCDM model in terms of significances.\nModels ∆ χ2\nDESY5 ∆AICDESY5 ∆χ2\nPantheon+ ∆AICPantheon+\nCKN with\nΛCDM −6.90 −6.90 −2.05 −2.05\nωCDM 3 .14 1 .14 2 .26 0 .26\nω0ωaCDM 5 .74 1 .74 2 .43 −1.57\nνCKN with\nΛCDM −6.94 −4.94 −3.07 −1.07\nωCDM 3 .09 3 .09 1 .24 1 .24\nω0ωaCDM 5 .69 3 .69 1 .41 −0.59\nTable 4: Difference of the χ2\nmin values ∆χ2\nDR2−DR1 between the fit with DESI BAO DR1 and DR2 combined\nwith Hubble measurements and either the DESY5 or the Pantheon+ supernova dataset for different cosmological\nmodels considered in this work.\nModels ∆χ2\nDR2−DR1\nDESY5 Pantheon+\nCKN −2.85 −2.84\nνCKN −2.91 −3.76\nΛCDM −0.52 −1.93\nωCDM −3.72 −3.59\nω0ωaCDM −3.29 −3.13\n3",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_8",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-cecbe07346956de913bd2968697311ae": {
    "tokens": 203,
    "content": "the commitment assumption in pricing [ 36] or restricting the strategy space in some way [ 20, 21].\nIn settings with restricted commitment or strategy spaces, learning is generally possible. However,\nin contrast to this literature we are interested in settings beyond the standard pricing problem,\nand we demonstrate that even in settings with full strategic power and symmetric discount rates,\ndeviations from the classic pricing problem lead to the ability to learn effectively.\n1.2.3 Other Related Work\nA more recent literature has developed around algorithmic contract design problems. Most of this\nliterature has focused on the computational issues of contract design [ 2, 14, 15, 24]. In recent work,\nthe sample complexity of online contract design has been explored by [ 34, 64]. [ 17] further extend\nthe analysis to a specific scenario where the agent’s utility function exhibits bounded risk aversion.\nHowever, this learning does not happen in a strategic setting, which is our main focus.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_37",
    "file_path": "CSE_1.pdf"
  },
  "chunk-dd510018dce062f3dfefda5422f2f3cd": {
    "tokens": 96,
    "content": "For this section, we will be primarily be concerned with ( m − 1)-dimensional hyperplanes that\ndefine the sets of leader strategies for which a follower of type θ ∈ Θ is indifferent between two\nactions i, j∈ [n]. These hyperplanes are defined by the sets Hθ\ni,j = {x ∈ Rm | x · (Cθ\n·,i − Cθ\n·,j) = 0},\n12",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_67",
    "file_path": "CSE_1.pdf"
  },
  "chunk-80bd1e7c81c6557ae6767daa67a2cbf3": {
    "tokens": 251,
    "content": "Proof. Proof of the connection between MILP (45) and MIP (47). The proof has two steps. We first\nshow that any optimal solution for MIP (47) must correspond to a feasible solution of MILP (45)\nwith the same objective value, and then show its reverse direction.\nStep 1: OP T(45) ≥ OP T(47). Consider any optimal solution x, y, and a for MIP (47). We\nshow that x, y, a, together with constructed z, w via Equations (52), forms a feasible solution of\nMILP (45). This follows from relatively standard algebraic derivations. Consider any optimal solution\nx, y, and a for MIP (47). We argue that x, y, a, together with constructed z, w via Equations\n(52), forms a feasible solution of (45). The equivalence of the objective function is immediately\nsatisfied by construction. It is also obvious that 0 ≤ z ≤ 1 and 0 ≤ w ≤ 1 by construction. Next,\nwe show that constraints (45b) – (45c) are satisfied. Recall that each entry of y is binary. For any",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_167",
    "file_path": "CSE_1.pdf"
  },
  "chunk-42c3af9416974a16b30bb2a45acf5534": {
    "tokens": 238,
    "content": "Now we can state our sufficient condition to ensure that learning is effective for a dynamic\nBayesian Stackelberg game.\nAssumption 1 (Existence of Learnable Sub-group) . Given BSE leader strategy x∗, there exists a\nsub-group Θ′ ⊂ Θ such that BSE(Θ′) ̸= x∗ and BR(Θ′, x∗) ∩ BR(Θ \\ Θ′, x∗) = ∅.\nAs we will show in Theorem 2, Assumption 1 is a sufficient condition to ensure that there exists\na policy that learns effectively. Assumption 1 states that there exists a subgroup whose set of best\nresponses is entirely disjoint from the rest of the follower types. Unsurprisingly, the pricing game\ndoes not satisfy Assumption 1. In the pricing game, at the BSE, there is always one type that is\nindifferent between purchasing and not purchasing. Therefore, there is no best response set that is\ndisjoint from the other types.\nIt may seem a-priori reasonable that a strategic leader should be able to induce a subgroup with",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_63",
    "file_path": "CSE_1.pdf"
  },
  "chunk-d13143b2df76e6a33535157d46630e4e": {
    "tokens": 224,
    "content": "Figure 4: Illustration of the Sequence of Leader Strategies. The leader first plays a strategy for\nt-rounds to elicit the followers type. Then the leader plays each strategy in the randomized set of\nstrategies for approximately (T − t)p∗\nθ,j-rounds.\nStep 1: Eliciting follower type θ. This phase takes t = ⌈logn(|Θ|)⌉ rounds. Construct\nfunction H : Θ → [n]t such that H(θ) equals precisely the length- t bit sequence of the n-nary\nrepresentation of θ (interpreting θ as an integer equaling at most |Θ|). By definition, each θ has\na unique H(θ) value. The leader strategies in these first t rounds can be arbitrary. The main\nchallenge is deferred to the second phase below, during which we will carefully design the strategies\nto incentivize each follower type θ to respond exactly with action sequence H(θ) in this first phase.\nStep 2: Constructing an approximately-optimal strictly-IC randomized menu. There",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_146",
    "file_path": "CSE_1.pdf"
  },
  "chunk-01f87558da332d7940f37a574f96e4e4": {
    "tokens": 319,
    "content": "of the simplex ∆ m, {x | xi = 0}i∈[m], are added to the set, they are still in general position with\nprobability one. There are two cases that must be considered either the BSE(Θ), denoted x∗, is at\nthe vertex of the simplex ∆ m, i.e. there exists some i ∈ [m] such that x∗\ni = 1, or x∗ is not at the\nvertex of the simplex. We will first consider the case where x∗ is not at the vertex of the simplex.\nIn this case, x∗ must be at the intersection of exactly m distinct hyperplanes, where at least one of\nthose hyperplanes is Hθ∗\ni,j for some θ∗ ∈ Θ and i, j∈ [n]. This is due to x∗ not being at the vertex\nof the simplex and the leader’s objective function being linear.\nBy Assumption 2 and Theorem 3, with probability greater than 1−O\n\u0010\n1\n|Θ|\n\u0011\nthere exists a Θ′ ⊂ Θ\nsuch that BR(Θ′) ∩BR({θ∗}) = ∅. Note that this implies that θ∗ /∈ Θ′. Therefore, BSE(Θ′) ̸= x∗ since\nthe hyperplane Hθ∗\ni,j is not in the set of indifference curves for Θ ′.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_140",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9886c2a9345c7f13fb7e166a840f6a3c": {
    "tokens": 217,
    "content": "within two rounds in the battle of sexes game while in the SSG and game of chicken, UDSE did not\nconverge within five rounds. The convergence of UDSE is an interesting open question for future\nresearch.\nExperimental Results on Randomized Games. We additionally conducted experiments\non random game instances in which each agent’s utility is uniformly drawn from [0 , 1]. As in the\nprevious experiments, we compare the average leader utility between DSE and the optimal static\npolicy. In addition, to demonstrate the performance of our approximation algorithms, we also\ncompare the runtime and average leader utility of solving the optimal Markovian policy and\nFirst-k policy, versus solving the DSE.\nThe results from T=1 , 2, 3 have been averaged over 50 random instances and the standard\ndeviation is reported in the table. As for T ≥ 4, the runtime is high, and the advantage of First-k\nand Markovian policy is significant (note the runtime of DSE when T = 4 is already much higher",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_98",
    "file_path": "CSE_1.pdf"
  },
  "chunk-40be5799e1d3930b8dd9c72804a4e617": {
    "tokens": 218,
    "content": "drawn from a known distribution, i.e. binary demand, it is impossible for a seller to exploit any\ninformation learned to improve her revenue. Formally, no dynamic pricing policy can outperform\nthe fixed (or static) policy of simply offering the optimal single round price, i.e. the Myerson price\n[45], at every round, at least when the buyer is both strategic and patient. 1\nThis negative result, which we refer to as the No Learning Theorem, is a function of the buyer\nbehaving strategically in his purchase decisions in order to influence future prices. The effect on the\nliterature of this strong negative result has been to adopt assumptions that weaken the strategic\nbehavior of the buyer (e.g., myopic buyers) in order to obtain positive results [4, 5, 20, 21, 36, 57].\nGiven this foundational negative result, a natural question arises, “Does a static policy suffice to\nachieve optimality in a dynamic setting with strategic agents?” Stated another way, “Generally,",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_19",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b8b2dca5035bb10c85fa7338aa4a43e3": {
    "tokens": 251,
    "content": "Equilibrium (BSE). Unlike the SSE, computing a BSE in Bayesian Stackelberg games is NP-hard\n[18]. For notational convenience, we drop the leader utility’s dependence on the follower type θ\nand simply use R, U(x) instead of Rθ, Uθ(x). All results generalize trivially. We denote a specific\nBayesian Stackelberg game as {R, Θ, {Cθ}θ∈Θ, µ}.\n2.1.2 Dynamic Bayesian Stackelberg Games\nIn this subsection, we formalize the concept of a dynamic Bayesian Stackelberg game. Specifically,\ndynamic Bayesian Stackelberg games generalize static (Bayesian) Stackelberg games by allowing\nrepeated leader-follower interactions where the agent responds non-myopically to maximize their\ncumulative utility throughout the repeated interactions. Formally, a Bayesian Stackelberg game,\n{R, Θ, {Cθ}θ∈Θ, µ}, is played repeatedly for T rounds. The follower has a fixed private type θ\ndrawn from µ. We denote a specific dynamic Bayesian Stackelberg game as {R, Θ, {Cθ}θ∈Θ, µ, T}.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_43",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e85717c5e853101f9733c4694ebb8ad9": {
    "tokens": 290,
    "content": "A randomized menu with δ inducibility gap is called δ-strictly IC.\nTheorem 4. For any Stackelberg game with inducibility gap δ and T ≥ Ω(log2\nn |Θ|), we have\nUDSE\nT ≥ URME − O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\n. Moreover, there exist instances where UDSE\nT ≤ URME − Ω\n\u00001\nT\n\u0001\n.\nFor a proof of Theorem 4 see Appendix C. Theorem 4 shows that DSE achieves equivalent utility\nto the RME, up to a O\n\u0010q\n1\nT\n\u0011\ngap. It turns out that one can easily find examples in which RME will\nbe significantly worse — specifically, Ω(1) worse — than the average DSE utility, even for policies\nthat strictly rely on learning and not commitment. Consider the following example.\nExample 4. Define a Bayesian Stackelberg game with the utility matrices as represented in Table 5\nwhere the probability of each follower type is 1\n2.\nIn Example 4, the set of RME’s consists of the following menus {1, (1, 0)} and {1, (0, 1)}, i.e. the\nleader plays either action 0 or 1 with probability 1. This optimal static strategy with communication",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_84",
    "file_path": "CSE_1.pdf"
  },
  "chunk-a14f8979cea1649f05504d42a90454a0": {
    "tokens": 201,
    "content": "i∈[n]\n \\\nθ∈Θ\nBRθ(i)\n!\n̸= ∅\n!\n≤\nnX\ni=1\nP\n \\\nθ∈Θ\nBR′\nθ(i) ̸= ∅\n!\n≤ n\n\n 1\n|Θ| + 2 m\n|Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n+ m\n\n = O\n\u0012 1\n|Θ|\n\u0013\nfor a fixed n and m.\nUsing Theorem 3, we can prove Corollary 1.\nProof. Proof of Corollary 1. First, note that by Assumption 2 the set of hyperplanes Hθ\ni,j for all\ni, j∈ [n] and θ ∈ Θ are in general position. Additionally, if the hyperplanes that define the boundary\n33",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_139",
    "file_path": "CSE_1.pdf"
  },
  "chunk-d3a7088196faa1bbde25bcb03050b547": {
    "tokens": 183,
    "content": "The principal aims to maximize their worst-case utility for all distributions within a specified set. In\ncontrast, our setting focuses on maximizing the principal’s average utility given a prior distribution\nover agent types. Interestingly, our results differ from those of Balseiro et al . [8]. Specifically, they\nshow that, under two assumptions, the optimal static mechanism achieves the minimax utility\nfor the principal, whereas we demonstrate that dynamic policies increase average utility for most\ndynamic Bayesian Stackelberg games. Our setting satisfies their two assumptions, suggesting that\nthe fundamental difference lies in the maximin versus expected utility formulations. This implies\nthat, in settings with a prior over agent types, learning is likely effective, while in settings with a\nmaximin objective—often used for robustness—learning is generally not effective.\nOur setting, if viewed as a restricted dynamic mechanism design problem, is also related to Pavan",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_29",
    "file_path": "CSE_1.pdf"
  },
  "chunk-308f90d7e1bf0acffadf9ebd8ea3b502": {
    "tokens": 170,
    "content": "Game of Chicken. The game of chicken models situations in which two players desire a\nshared resource but conflict arises when both players choose to use the resource simultaneously.\nWe consider a variant of this game with a leader-follower structure and two uniformly distributed\nfollower types C0, C1 as given in Figure 1. The first row/column action represents “giving up the\nresource” whereas the second row/column action presents “using the resource”. Type C0 is the\ntype as in the classic game of chicken whereas C1 is an “aggressive” type who strongly prefers using\nthe resource.\nStackelberg Security Game (SSG). We consider a specific SSG, where there is a defender\n(leader) trying to protect 2 targets with 1 resource from the attack of an attacker (follower).\n18",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_93",
    "file_path": "CSE_1.pdf"
  },
  "chunk-76551418809f9a3b87d401bd79e849aa": {
    "tokens": 647,
    "content": "due to the independence across types in Assumption 2. Consider the first term in equation 33.\nDenote by K = χ\n\u0010T\nθ′∈Θ′ Hθ′,+\ni,j ∩ Hθ\ni,j\n\u0011\nBy the previous discussion, again using the law of total\nprobability and noting that the Euler characteristic K = χ\n\u0010T\nθ′∈Θ′ Hθ′,+\ni,j ∩ Hθ\ni,j\n\u0011\nwill be 1 only if\nT\nθ′∈Θ′ Hθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅:\nY\nθ∈Θ\\Θ′\nP\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅ |J\n!\n=\nY\nθ∈Θ\\Θ′\n \nP (K | J) + P (¬K | J) P\n \\\nθ′∈Θ′\nHθ′,+\ni,j ∩ Hθ,+\ni,j ̸= ∅ |J, ¬K\n!!\n≤\nY\nθ∈Θ\\Θ′\n\u0012\ndp + (1 − dp)\n\u00121\n2\n\u0013\u0013\n=\n\u00121 + dp\n2\n\u0013|Θ|−|Θ′|\n.\nTherefore\nP\n \\\nθ∈Θ\nBR′\nθ(i) ̸= ∅\n!\n≤\n\u00121 + dp\n2\n\u0013|Θ|−|Θ′|\n+ 1\nd.\nSet d = 1\n2p and choose a set Θ ′ such that |Θ′| = |Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n. Given that p = m\n|Θ′|+m then\n\u00121 + dp\n2\n\u0013|Θ|−|Θ′|\n+ 1\nd =\n\u00123\n4\n\u0013\n\u0018\nlog 3\n4\n1\n|Θ|\n\u0019\n+ 2 m\n|Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n+ m\n≤\n\u00123\n4\n\u0013log 3\n4\n1\n|Θ|\n+ 2 m\n|Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n+ m\n= 1\n|Θ| + 2 m\n|Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n+ m\nTherefore\nP\n [\ni∈[n]\n \\\nθ∈Θ\nBRθ(i)\n!\n̸= ∅\n!\n≤\nnX\ni=1\nP\n \\\nθ∈Θ\nBR′\nθ(i) ̸= ∅\n!\n≤ n\n\n 1\n|Θ| + 2 m\n|Θ| −\nl\nlog3\n4\n1",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_138",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5cb71b29b305924745b3a84556d97399": {
    "tokens": 196,
    "content": "studied the optimal mechanism for a two-period ticket-selling problem. In contrast to our work,\ntheir setting involves an agent who only knows the distribution of their valuations in the first period\nand learns their actual valuations in the second period. In our setting, the agent knows their type\nfrom the beginning, and it remains constant throughout all rounds. Battaglini [10] examined the\nproblem of characterizing the optimal contract between a firm and a long-term customer. They show\nthat when the customer’s type is not constant, the optimal contract is not static. In our setting,\nwe look at the case where the follower’s type is constant, and the optimal policy is, in general, not\nstatic. Overall, our work primarily differs in that we focus on the prevalence of learning in strategic\nsettings, while the previously mentioned works either broadly characterize optimal mechanisms [ 49]\nor investigate the possibility of learning in specific instances [9, 10, 19].",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_31",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ecbab80626a21e1e63d9478499ff1b79": {
    "tokens": 192,
    "content": "that for a randomly generated Bayesian Stackelberg game the probability that effective learning\nis possible is increasing in the number of follower types, given some mild assumptions on the\ngame generation process. This suggests that effective learning being possible is, in some sense, the\nmost likely state. Moreover, our definition of a random Bayesian Stackelberg game only assumes\nrandomness over the followers payoffs. For the leader, we simply need the condition that there is\nnot a dominant action, i.e. an action for which, no matter the followers response, is at least as\ngood as any other action. Clearly, learning cannot be effective if there is a dominant action, so this\ndemonstrates that it is sufficient for the follower’s payoff matrices to be “general” in order to imply\nthat learning is effective with high probability.\nFor this section, we will be primarily be concerned with ( m − 1)-dimensional hyperplanes that",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_66",
    "file_path": "CSE_1.pdf"
  },
  "chunk-46fab8ab8e67e529be05dfcbe124edf3": {
    "tokens": 211,
    "content": "Stackelberg games?\nAs the following examples illustrate, it is indeed possible to learn effectively from a strategic follower,\neven when the dynamic Stackelberg game is a small modification of a dynamic pricing game.\nExample 1. Let a Bayesian Stackelberg game be defined by the payoffs listed in Table 1, and assume\nthat types 0 and 1 are equally likely. As can be computed, the optimal static policy for the leader in\nthis game is the ( BSE) leader strategy x = (1\n3, 2\n3), i.e. the leader commits to a randomized strategy\nwhich plays action i0 with probability 1\n3 and i1 with probability 2\n3. This BSE strategy results in an\nexpected leader utility of 51\n6.\nIn Example 1, consider the simplest possible dynamic setup with two rounds of leader-follower\ninteractions. The optimal dynamic leader policy is to play x1 = (0, 1) in the first round. If the\n8",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_50",
    "file_path": "CSE_1.pdf"
  },
  "chunk-90082f5b6a23791d9d9be2d08b55831b": {
    "tokens": 157,
    "content": "Similarly, constraint (10) ensures that all types not in Θ ′ do not find it optimal to deviate and\npretend to be in Θ ′. Note that by Assumption 1 for all j ∈ BR(Θ′) we have j /∈ BR(Θ \\ Θ′),\nV θ(x∗, j∗θ(x∗)) > max\nj∈BR(Θ′)\nV θ(x∗, j)\nSimilarly, equation (10) will be satisfied if the following holds:\n(T − 2) · (V θ(x∗, j∗θ(x∗)) − max\nj /∈BR(Θ′)\nV θ(x∗, j)) ≥ 1\n27",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_123",
    "file_path": "CSE_1.pdf"
  },
  "chunk-56b3c561850d4c68b4141e7860526ded": {
    "tokens": 274,
    "content": "[10] Marco Battaglini. 2005. Long-term contracting with Markovian consumers. American Economic\nReview 95, 3 (2005), 637–658.\n[11] Eyal Beigman and Rakesh Vohra. 2006. Learning from revealed preference. In Proceedings of\nthe 7th ACM Conference on Electronic Commerce . 36–42.\n[12] Alain Bensoussan, Shaokuan Chen, and Suresh P Sethi. 2015. The maximum principle for\nglobal solutions of stochastic Stackelberg differential games. SIAM Journal on Control and\nOptimization 53, 4 (2015), 1956–1981.\n[13] Patrick Bolton and Mathias Dewatripont. 2004. Contract theory. MIT press.\n[14] Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. 2021. Bayesian agency: Linear\nversus tractable contracts. In Proceedings of the 22nd ACM Conference on Economics and\nComputation. 285–286.\n[15] Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. 2022. Designing Menus of Contracts\nEfficiently: The Power of Randomization. In EC ’22: The 23rd ACM Conference on Economics",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_105",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b887317721bb8c6685d21025267890ca": {
    "tokens": 266,
    "content": "stochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 26.\n1478–1484.\n[61] Quoc-Liem Vu, Zane Alumbaugh, Ryan Ching, Quanchen Ding, Arnav Mahajan, Benjamin\nChasnov, Sam Burden, and Lillian J Ratliff. 2022. Stackelberg Policy Gradient: Evaluating\nthe Performance of Leaders and Followers. In ICLR 2022 Workshop on Gamification and\nMultiagent Solutions.\n[62] Jibang Wu, Weiran Shen, Fei Fang, and Haifeng Xu. 2022. Inverse Game Theory for Stackelberg\nGames: the Blessing of Bounded Rationality. In Advances in Neural Information Processing\nSystems.\n[63] Rong Yang, Benjamin J Ford, Milind Tambe, and Andrew Lemieux. 2014. Adaptive resource\nallocation for wildlife protection against illegal poachers.. In Aamas. 453–460.\n[64] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I Jordan.\n2022. The Sample Complexity of Online Contract Design. arXiv preprint arXiv:2211.05732\n(2022).",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_116",
    "file_path": "CSE_1.pdf"
  },
  "chunk-745ec6a4e739816a11fe84c7c4fe5f07": {
    "tokens": 332,
    "content": "the hyperplane Hθ∗\ni,j is not in the set of indifference curves for Θ ′.\nNow, we consider the case where x∗ is a vertex point of ∆ m. We will denote by xi the strategy\nxi = 1 and xj = 0 for all j ∈ [m] \\ {i′}. Therefore, there exists an i∗ ∈ [m] such that x∗ = xi∗\n.\nFirst, note that there are m vertices, and each vertex is the intersection of exactly m hyperplanes.\nMoreover, with probability one, each follower type θ ∈ Θ will have exactly one best response, due to\nthe set of hyperplanes Hθ\ni,j for all i, j∈ [n] and {x | xi = 0}i∈[m] being in general position. In this\ncase, by the assumption that leader action i∗ ∈ [m] is not a dominant strategy, there exists some\nfollower action j′ ∈ [n] and some other leader action i′ ∈ [m] such that Ri∗,j′ ≤ Ri′,j′. Therefore, if\nthere is some subset of types Θ ′ ∈ Θ such that BR(Θ′) = {j′}, then BSE(Θ′) ̸= xi∗\nsince the strategy\n(1 − ϵ)xi∗\n+ ϵxj′\nis a strict improvement for a sufficiently small ϵ, given that the indifference curves",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_141",
    "file_path": "CSE_1.pdf"
  },
  "chunk-3233efd98046d37d5b678056459fe230": {
    "tokens": 532,
    "content": "UDSE/T URME\nT = 1 1/3 1/3\nT = 2 0.444 1/3\nT = 3 0.467 1/3\nT = 4 0.479 1/3\nT = 5 0.493 1/3\nTable 10: Average Leader Utility Per Round. The average utility per round for both the DSE and\nRME is shown for T = 1, ...,5.\nRuntime Average Utility\nFirst−k Markovian DSE First−k Markovian DSE RME\nT = 1 0.02 ± .002 0.02 ± .004 0.02 ± .004 0.66 ± .12 0.66 ± .12 0.66 ± .12 0.72 ± .12\nT = 2 0.44 ± 0.3 0.48 ± 0.4 0.44 ± 0.28 0.73 ± .11 0.73 ± .11 0.73 ± .11 0.72 ± .12\nT = 3 6.5 ± 1.9 2.4 ± 1.4 6.6 ± 1.9 0.76 ± .11 0.75 ± .11 0.76 ± .11 0.72 ± .12\nT = 4 79 ± 49 11 ± 11 79 ± 49 0.78 ± .11 0.76 ± .11 0.78 ± .11 0.72 ± .12\nT = 5 104 ± 48 75 ± 85 1247 ± 1031 0.78 ± .11 0.76 ± .11 0.79 ± .11 0.72 ± .12\nT = 6 125 ±46 658 ±1060 N/A 0.78 ± .11 0.76 ± .12 N/A 0.72 ± .12\nT = 7 141 ±65 6657 ± 1213 N/A 0.78 ± .11 0.77 ± .11 N/A 0.72 ± .12\nT = 8 160 ±59 N/A N/A 0.78 ± .11 N/A N/A 0.72 ± .12\nTable 11: Additional experimental results on randomized games. Running time (columns 2-4 with",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_177",
    "file_path": "CSE_1.pdf"
  },
  "chunk-95bba82f38b3d161cfdb7d243130147a": {
    "tokens": 134,
    "content": "The optimal expected utility at the first round, however, is at most 0 .5. In this case, we have\nUDSE = 0.5 + (T − 1) while playing RME gives the leader a utility of T, if the leader and the follower\ninteract for T rounds. Therefore, we get UDSE\nT = URME − 1\n2T , proving the theorem.\nD Omitted Details in Section 5\nD.1 Proof of Theorem 5\nHere we will show that an optimal solution to Program (2) can be recovered from an optimal\nsolution to the following MILP:\n38",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_155",
    "file_path": "CSE_1.pdf"
  },
  "chunk-75a672e150204a74a0d91daae672ad95": {
    "tokens": 171,
    "content": "Taken together, these results suggest that weakening the follower’s strategic space to obtain positive\nlearning outcomes, as has commonly been done in the literature, may be unnecessary outside of\ndynamic pricing settings.\nMoving forward, several open questions remain. The most immediate is whether there exists a\nsimple characterization of a necessary and sufficient condition to ensure effective learning. While\nwe provide a sufficient condition, it is easy to construct examples, such as Example 3, where our\nsufficient condition is not satisfied, yet learning is still effective. Additionally, we show that for\nrandom Bayesian Stackelberg games, learning is effective with high probability relative to the BSE.\nWe also demonstrate that learning is at least as powerful as communication (Theorem 4), and by\nexample, we show that learning can be arbitrarily more powerful than communication. However, we",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_101",
    "file_path": "CSE_1.pdf"
  },
  "chunk-15df020dd5aa2a715ed8aae18a266fa2": {
    "tokens": 264,
    "content": "2022. The Sample Complexity of Online Contract Design. arXiv preprint arXiv:2211.05732\n(2022).\nA Discussion of the No-Learning Theorem in Dynamic Pricing\nThe key technique for proving the optimality of static constant pricing is an elegant reduction from\ndynamic pricing interactions to a single-round feasible auction mechanism [ 45]1. Specifically, for\nany pricing policy π, we construct an auction mechanism Mπ that works as follows. First, the seller\nasks the buyer to report his value v and simulates j∗ based on the reported v, denoted as:\nj∗(v) = argmax\nj∈{0,1}T\nX\nt∈[T]\njt ·\n\u0000\nv − π(j1, ··· , jt−1)\n\u0001\n. (3)\nThen the seller randomly chooses a round t ∈ [T] with probability 1/T and allocates the item to\nthe buyer with price π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\nif j∗\nt (v) = 1.\n1Details about feasible auction mechanisms can be found in Section 3 of [45]\n25",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_117",
    "file_path": "CSE_1.pdf"
  },
  "chunk-8ffcbbb45b3ddd00bca205822d07a549": {
    "tokens": 224,
    "content": "It is straightforward to verify that for type 0 it is always a best response to play j0 and for type\n1 it is always a best response to play j1. This leads to an expected first round utility of 1\n2, and\nin all future rounds, the expected utility is 1, a total expected utility for the T rounds of T − 1\n2.\nTherefore, there is a gap of 1\n2(T − 1) between the RME and this dynamic strategy.\nImportantly, this increase in utility is driven by learning, not commitment. Stated differently, if\nthe leader knew the follower’s type, the optimal dynamic policy would be a static policy. In this\nexample, the ability to commit does not increase the leader’s utility under full knowledge. Therefore,\nit is learning that is driving the increase in utility relative to the RME.\n5 Computing the DSE Exactly and Approximately\nIn preceding sections we have demonstrated that in general there exists a dynamic policy which\nlearns effectively for dynamic Bayesian Stackelberg games and that this learning is not merely",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_86",
    "file_path": "CSE_1.pdf"
  },
  "chunk-85b933cee0c21a7a17fda0aedf96e733": {
    "tokens": 427,
    "content": "and equivalently, X\nj\npθ,jV θ(xθ,j, j) ≥\nX\nj\npθ′,j max\nj′\nV θ(xθ′,j, j′), ∀θ, (42)\nproving the new randomized menu ⟨p, x⟩ is IC.\nFinally, the leader’s utility of the new randomized menu ⟨p, x⟩ is also bounded by Lemma 4 for\nall θ ∈ Θ as follows\nX\nj\npθ,jU(xθ,j, j) ≥\nX\nj\npθ,jU(xθ,j, j) − ϵ =\nX\nj\npθ,jU(xθ,j, j) − O\n r\nlog |Θ|\nT\n!\n. (43)\nSince by construction, the original randomized menu ⟨p, x⟩ is O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\noptimal, we have for all\nθ ∈ Θ\nX\nj\npθ,jU(xθ,j, j) ≥\nX\nj\np∗\nθ,jU(x∗\nθ,j, j) − O\n r\nlog |Θ|\nT δ2\n!\n. (44)\nCombining (43) and (44), we also have the new randomized menu ⟨p, x⟩ is O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\noptimal,\nproving Step 3. Since a randomized menu with ( T − t)-uniform distribution can be precisely\nsimulated by a dynamic policy whereas UDSE denotes the leader utility under the optimal dynamic\npolicy, we have UDSE\nT ≥ u⟨p,x⟩\nl and proved UDSE\nT ≥ URME − O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\nin the Theorem.\nInstance with Ω(1/T) lower bound. We remark that the upper and lower bound of the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_153",
    "file_path": "CSE_1.pdf"
  },
  "chunk-808b9fcc36e382ab1a478a010159ca1e": {
    "tokens": 267,
    "content": "The final step is to ensure that for this subgroup the leader’s optimal strategy,BSE(Θ′) ̸= x∗. The\nfollowing Corollary ensures that this is the case with high probability. The proof is in Appendix B.2.\nHowever, we must additionally assume that the leader does not have a strictly dominant action, i.e.\nthere does not exist a row of Ri,· of the leader’s utility matrix R that dominates every other row. If\nthere is such a row, learning cannot be effective since the leader should always just play action i.\nCorollary 1. Let {R, Θ, {Cθ}θ∈Θ, µ, f} be a random Bayesian Stackelberg game that satisfies\nAssumption 2 with µ having full support on Θ. Additionally, let the leader’s payoff matrix R ∈ Rm×n\nbe such that there does not exist a row Ri,· of R such that for all j ∈ [m] \\ i, Ri,· is element-wise\ngreater than or equal to Rj,·; i.e., there does not exist a dominant action for the leader. Then the\nprobability that Assumption 1 is not satisfied is O\n\u0010\n1\n|Θ|\n\u0011\n.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_74",
    "file_path": "CSE_1.pdf"
  },
  "chunk-51931485376d2ce3e0a13abeba9a979c": {
    "tokens": 219,
    "content": "Figure 1: Utility Matrices for the Game of Chicken. A leader-follower variant of the game of\nchicken with two follower types. The average utility per round for both the DSE and RME is shown\nfor T = 1, ...,5.\nSpecifically, there are two uniformly distributed follower types whose utility information is given\nby Figure 2. Each row represents the leader’s action (i.e. protecting the target) and each column\nrepresents the follower’s action (i.e. attacking the target). Follower type V 0 prefers target t0 while\nC1 prefers target t1.\nFigure 2: A Stackelberg Security Game with two follower types. The average utility per round for\nboth the DSE and RME is shown for T = 1, ...,5. Utility Matrices for a Stackelberg Security Game.\nBattle of the Sexes. In this game, two players receive higher utilities when they take the\nsame action, though one of them enjoys this action more than the other. We consider a variant of",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_94",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7a77212de106ee8c3937cc19c1605090": {
    "tokens": 476,
    "content": "As a result, we have the expected utility of ⟨p, x⟩ as U⟨p,x⟩ ≥ (1 −\nq\nlog |Θ|\nT δ2 )URME, proving the\nlemma.\nStep 3: Existence of (T − t)-uniform, O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\noptimal, and IC menu.\nLemma 4 (Alth¨ ofer[3]). For any ϵ >0 and any {pθ,j, xθ,j}j∈[n], there exists a k-uniform pθ with\nk =\nl\nlog 2(|Θ|+1)\n2ϵ2\nm\nsuch that\nϵ ≥\n\f\f\f\f\f\f\nX\nj\npθ,jU(xθ,j, j) −\nX\nj\npθ,jU(xθ,j, j)\n\f\f\f\f\f\f\n,\nand for all θ′\nϵ ≥\n\f\f\f\f\nX\nj\npθ,j max\nj′\nV θ′\u0000\nxθ,j, j′\u0001\n−\nX\nj\npθ,j max\nj′\nV θ′\u0000\nxθ,j, j′\u0001\f\f\f\f.\nBy Lemma 4, there always exists an approximation of pθ denoted as pθ for all θ, which is a\n(T − t)-uniform distribution and can be precisely fulfilled in the T − t rounds. Next, we show that\nthe new randomized menu ⟨p, x⟩ is IC and O(\nq\nlog |Θ|\nT δ2 ) optimal.\nSince the original ⟨p, x⟩ is O(\nq\nlog |Θ|\nT )-strictly IC randomized menu, we have\nX\nj\npθ,jV θ(xθ,j, j) ≥\nX\nj\npθ′,j max\nj′\nV θ(xθ′,j, j′) + O(\nr\nlog |Θ|\nT ), (37)\nfor all θ and θ′ ̸= θ. When we approximate pθ with a (T − t)-uniform distribution pθ, there exists",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_151",
    "file_path": "CSE_1.pdf"
  },
  "chunk-723e3333125f1b60c29bd456d6ad9b7e": {
    "tokens": 287,
    "content": "θ shows up in the indices of the leader’s decision variables xt\njt−1. Our main goal\nin this step is to remove this dependence. To do so, we introduce a different variable representation\nfor the follower’s action space. Specifically, we use a binary matrix yθ ∈ {0, 1}T×n to represent the\nresponse sequence for any follower type θ ∈ Θ.\nMoreover, the t’th row yθ\nt ∈ {0, 1}n is a one-hot vector of length n, in which the index of the 1\nentry represents the action taken by the follower at time step t. The key step of the reduction is to\ncharacterize the follower’s optimal response history by the following constraint 2\n0 ≤ aθ −\nX\nt,i\nxt\njt−1,iCθ\ni,jt ≤ M(T −\nTX\nt=1\nyθ\nt,jt), ∀jT , θ (46)\nwhere M is a very large constant and a ∈ Rk is a set of newly introduced decision variables. Formally,\nProgram (2) can be transformed into aMIP with an objective ofP\nθ µ(θ) P\nt,jt,i Ri,jt xt\njt−1,i\nQt\nt′=1 yθ\nt′,jt′ .",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_157",
    "file_path": "CSE_1.pdf"
  },
  "chunk-8ec6368c7698fb8e0c15be5204927c61": {
    "tokens": 282,
    "content": "[20] Quinlan Dawkins, Minbiao Han, and Haifeng Xu. 2021. The Limits of Optimal Pricing in the\nDark. Advances in Neural Information Processing Systems 34 (2021), 26649–26660.\n[21] Quinlan Dawkins, Minbiao Han, and Haifeng Xu. 2022. First-Order Convex Fitting and\nIts Application to Economics and Optimization. In Proceedings of the AAAI Conference on\nArtificial Intelligence, Vol. 36. 6480–6487.\n[22] Yuan Deng, Jon Schneider, and Balasubramanian Sivan. 2019. Strategizing against No-regret\nLearners. In Advances in Neural Information Processing Systems . 1577–1585.\n[23] Nikhil R Devanur, Yuval Peres, and Balasubramanian Sivan. 2014. Perfect bayesian equilibria\nin repeated sales. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete\nalgorithms. SIAM, 983–1002.\n[24] Paul D¨ utting, Tim Roughgarden, and Inbal Talgam-Cohen. 2019. Simple versus optimal\ncontracts. In Proceedings of the 2019 ACM Conference on Economics and Computation . 369–\n387.\n22",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_107",
    "file_path": "CSE_1.pdf"
  },
  "chunk-a6bd5fd2b7e9dd3cd60225b1836dc2e7": {
    "tokens": 187,
    "content": "generally no direct information asymmetry. While play proceeds stochastically, the leader is not\ndirectly learning from the actions of the follower, distinct from our setting where the information\nasymmetry is core to the problem.\nFinally, our setting is reminiscent of a traditional bandit problem [55]. However, the key difference\nbetween the traditional bandit problem and our setting is that the arm does not behave strategically.\nThe strategic consideration is core to our focus. Therefore, traditional no-regret learning results\ndo not apply in our setting, and in fact, the No Learning Theorem states that for at least some\nsettings (such as the dynamic pricing problem) the regret is unbounded.\n2 Preliminaries and Problem Setup\nIn this section, we introduce notation and we formally define the notion of a dynamic Bayesian\nStackelberg game along with the optimal solution concept.\n2.1 Stackelberg Games",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_39",
    "file_path": "CSE_1.pdf"
  },
  "chunk-afe4a40442d29910005cfb939d1b3496": {
    "tokens": 48,
    "content": "[38] Jean-Jacques Laffont and Jean Tirole. 1988. The dynamics of incentive contracts. Econometrica:\nJournal of the Econometric Society (1988), 1153–1175.\n23",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_111",
    "file_path": "CSE_1.pdf"
  },
  "chunk-776434eb746f3cd8037c98546b88f417": {
    "tokens": 455,
    "content": "maximize\nX\nθ\nµ(θ)\nX\nt,jt,i\nRi,jt wt,θ\njt,i (45a)\nsubject to\nzt,θ\njt ≤ yθ\nt,jt, z t,θ\njt ≤ zt−1,θ\njt−1 , z t,θ\njt ≥ yθ\nt,jt + zt−1,θ\njt−1 − 1, ∀jt ∈ [n]t, t∈ [T], θ∈ Θ, (45b)\nwt,θ\njt,i ≤ xt\njt−1,i, w t,θ\njt,i ≤ zt,θ\njt , ∀jt ∈ [n]t, t∈ [T], θ∈ Θ, i∈ [m], (45c)\nX\ni\nxt\njt−1,i = 1, ∀t ∈ [T], jt−1 ∈ [n]t−1, (45d)\nX\nj\nyθ\nt,j = 1, ∀t ∈ [T], θ∈ Θ, (45e)\n0 ≤ aθ −\nX\nt,i\nxt\njt−1,iCθ\ni,jt ≤ M(T −\nX\nt\nyθ\nt,jt), ∀jt ∈ [n]t, θ∈ Θ, (45f)\n0 ≤ z ≤ 1, 0 ≤ w ≤ 1, 0 ≤ x ≤ 1, y ∈ {0, 1}. (45g)\nD.1.1 A Mixed Integer Program ( MIP)\nOur first step is to reduce Program (2) to a Mixed Integer Program ( MIP). While this MIP still\ncannot be solved by industry-standard optimization solvers like Gurobi [30], it provides an important\nstep towards our final development of the MILP.\nAs mentioned above, the key barrier for practically solving Program (2) is that the follower’s\ndecision variables jt−1\nθ shows up in the indices of the leader’s decision variables xt\njt−1. Our main goal",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_156",
    "file_path": "CSE_1.pdf"
  },
  "chunk-034ad21bf068fa69b96451064e90908a": {
    "tokens": 97,
    "content": "sets C ∩ Sm−1 form a spherical tessalation of Sm−1. If we denote arbitrarily by H− one of the\ntwo half-spaces bounded by the hyperplane, then each of the m-dimensional cones defined by the\ntessalation T induced by H1, H2, ..., Hn are of the form:\nn\\\ni=0\nϵiH−\ni , ϵi = ±1 (12)\n28",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_127",
    "file_path": "CSE_1.pdf"
  },
  "chunk-bb6d7072ee781c26b274dabf9d84493c": {
    "tokens": 177,
    "content": "to a large literature on learning in strategic settings that relies on limiting the strategic space\nof the follower in order to provide positive results. In this paper, we study dynamic Bayesian\nStackelberg games, where a leader and a fully strategic follower interact repeatedly, with the\nfollower’s type unknown. Contrary to existing results, we show that the leader can improve their\nutility through learning in repeated play. Using a novel average-case analysis, we demonstrate\nthat learning is effective in these settings, without needing to weaken the follower’s strategic\nspace. Importantly, this improvement is not solely due to the leader’s ability to commit, nor does\nlearning simply substitute for communication between the parties. We provide an algorithm,\nbased on a mixed-integer linear program, to compute the optimal leader policy in these games\nand develop heuristic algorithms to approximate the optimal dynamic policy more efficiently.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_17",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ec34dff9e5cf28dc385d1f926d01f0ea": {
    "tokens": 431,
    "content": "R j0 j1\ni0 0 8\ni1 0 35\ni2 0 96\nC0 j0 j1\ni0 0 0\ni1 0 -27\ni2 0 -88\nC1 j0 j1\ni0 0 27\ni1 0 0\ni2 0 -61\nC2 j0 j1\ni0 0 88\ni1 0 61\ni2 0 0\nTable 2: Utility Matrices for Example 2. A pricing game example where R represents the seller’s\nutility matrix, and C0, C1, C2 represents the buyer’s utility matrices when his type is v = 8,\nv = 35, and v = 96 correspondingly. In addition, i0, i1, i2 represents setting a price of 8 , 35, 96,\nwhile j0/j1 represents the buyer rejects/accepts the price. For readability, we relax the assumption\nthat all utilities are in [0 , 1].\nR j0 j1\ni0 0 22\ni1 0 40\ni2 0 61\nC0 j0 j1\ni0 0 -14\ni1 0 -32\ni2 0 -53\nC1 j0 j1\ni0 0 13\ni1 0 -5\ni2 0 -26\nC2 j0 j1\ni0 0 74\ni1 0 56\ni2 0 35\nTable 3: Utility Matrices for Example 3. A pricing game example where R represents the leader’s\nutility matrix, and C0, C1, C2 represents the follower’s utility matrices when his type is v = 8,\nv = 35, and v = 96 correspondingly. In addition, i0, i1, i2 represents setting a price of 22, 40, 61",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_55",
    "file_path": "CSE_1.pdf"
  },
  "chunk-6508bffbda59e6360bc0fcae6a46e10f": {
    "tokens": 196,
    "content": "that the gap is minimal—up to an O\n\u0010\n1√\nT\n\u0011\ndifference. Specifically, we show that communication is\nnot strictly more advantageous, and that the Dynamic Stackelberg Equilibrium (DSE) can achieve\nutility that is approximately optimal compared to the static setting with communication. It is\nalso worth noting that when learning is ineffective in a dynamic setting, it implies that direct\ncommunication also offers limited improvement to the leader’s utility, again up to an O\n\u0010\n1√\nT\n\u0011\nfactor.\nBefore presenting our main result for this section, we introduce the concept of the optimal static\nleader utility in a Bayesian Stackelberg game that allows for communication.\n4.1 Randomized Menu Equilibrium ( RME)\nBy invoking the revelation principle of Myerson [46] for general principal-agent problems, the leader’s\noptimal static strategy is to offer an incentive compatible (IC) menu of randomized strategies. Such",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_79",
    "file_path": "CSE_1.pdf"
  },
  "chunk-bb5018208a3997c1814ad96abb7a8235": {
    "tokens": 150,
    "content": "and is visible in their corresponding correlation plots for the ( ν)CKN models. The widely discussed findings\nfrom DR1, that models featuring a time-varying dark energy are preferred with respect to ΛCDM, is further\nstrengthened by the recent data release of the DESI experiment. Remarkably, this trend is also visible in the\n(ν)CKN models. For the Pantheon+ dataset, the νCKN model experiences a larger improvement in the fit\nbetween DR1 and DR2 compared to the CKN model. The trend revealed through the new DESI data release\nfuels the hope that we are moving towards a future where the mysteries of dark energy may at last begin to\nunfold.\n2",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_6",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-5dcbe35042aefa49364aac3ea2a819b8": {
    "tokens": 317,
    "content": "R j0 j1\ni0 5 2\ni1 5 7\nC0 j0 j1\ni0 5 2\ni1 4 2\nC1 j0 j1\ni0 5 7\ni1 4 3\nTable 1: Utility Matrices for Example 1. A Bayesian Stackelberg game instance with two actions\nfor the leader and follower. The leader’s utility matrix is the first subtable R; the follower has two\npossible types, with utility matrix C0, C1 respectively, each having equal probability 0.5. For\nreadability, we relax the assumption that all utilities are in [0 , 1].\nfollower responds with j0 in the first round, the leader still plays x2 = (0, 1) in the second round;\notherwise, the leader plays x2 = (1/2, 1/2). Follower C0 best responds by playing j0 in both rounds,\ni.e. j∗0\n2 = (j0, j0), while C1 plays j1 in both rounds, i.e. j∗1\n2 = (j1, j1). This implies that the leader\nplays different strategies against the two types in the second round, i.e π(j∗0\n1 ) ̸= π(j∗1\n1 ); the optimal\npolicy requires that the leader learns, and exploits knowledge of, the followers type. Moreover, this",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_51",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7634808d0dacd5b4878163730ef37990": {
    "tokens": 416,
    "content": "t,jt) = 0 and aθ = P\nt,i xt\njt−1,iCθ\ni,jt by construction. Hence, the first constraint of (47)\nis also satisfied. The fact that yθ\nt,j = 0 guarantees that QT\nt′=1 yθ\nt′,jt′ = 0 if jT ̸= jθ\nT . Then we haveP\nt,jt,i Ri,j\n\u0000\nxt\njt−1,i\nQt\nt′=1 yθ\nt′,jt′\n\u0001\n= P\nt,i Ri,jθ\nt\nxt\njθ\nt−1,i = P\nt U(xt\njθ\nt−1\n, jθ\nt ), which shows the constructed\nfeasible solution of (47) has the same objective value as (2).\nLet us now consider x, y, and a feasible for (47). We construct jθ\nT such that jθ\nt ∈ [n] and\nyθ\nt,jθ\nt\n= 1. We will show that x and y are feasible for (2) with the same objective value. Recall\nthe discussion from equations (49)– (50), we have that jθ\nT captures the follower’s optimal behavior\nand satisfies P\nt∈[T] V θ(xt\njθ\nt−1\n, jθ\nt ) ≥ P\nt∈[T] V θ(xt\nbjt−1\n,bjt). What is more, by the same argument as\nin the previous direction, we have P\nt,jt,i Ri,j\n\u0000\nxt\njt−1,i\nQt\nt′=1 yθ\nt′,jt′\n\u0001\n= P\nt U(xt\njθ\nt−1\n, jθ\nt ), which shows\nthe equivalence between the objective values of these two programs.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_164",
    "file_path": "CSE_1.pdf"
  },
  "chunk-a62640ebf63572c074485fc07a254bab": {
    "tokens": 224,
    "content": "same action, though one of them enjoys this action more than the other. We consider a variant of\nthis game with a Stackelberg game structure and two uniformly distributed follower with payoff\nmatrics given by Figure 3. The C0 type is the standard player in the battle of the sexes game, but\nC1 is a type who “stubbornly” insists on the Basketball action even though they would enjoy it\nmore if the leader also picks this action.\nFigure 3: Utility Matrices for a Modified Battle of the Sexes Game. A Stackelberg variant of the\nBattle of the Sexes game with two follower types. The average utility per round for both the DSE\nand RME is shown for T = 1, ...,5.\nDiscussion on Structured Games. From the experimental results, we can see that the utility\nimprovement of DSE compared to RME is evident in all games. Note that when T = 1, the optimal\nDSE is exactly the BSE, which is always less than or equal to RME. We also highlight the increasing",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_95",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9f90962cacd9e0266547df86287ac68b": {
    "tokens": 324,
    "content": "[25] Xavier Freixas, Roger Guesnerie, and Jean Tirole. 1985. Planning under incomplete information\nand the ratchet effect. The review of economic studies 52, 2 (1985), 173–191.\n[26] Jiarui Gan, Minbiao Han, Jibang Wu, and Haifeng Xu. 2022. Optimal Coordination in Gener-\nalized Principal-Agent Problems: A Revisit and Extensions. arXiv preprint arXiv:2209.01146\n(2022).\n[27] Jiarui Gan, Minbiao Han, Jibang Wu, and Haifeng Xu. 2023. Robust Stackelberg Equilibria.\nIn Proceedings of the 24th ACM Conference on Economics and Computation .\n[28] Denizalp Goktas, Jiayi Zhao, and Amy Greenwald. 2022. Zero-Sum Stochastic Stackelberg\nGames. arXiv preprint arXiv:2211.13847 (2022).\n[29] Negin Golrezaei, Adel Javanmard, and Vahab Mirrokni. 2021. Dynamic Incentive-Aware\nLearning: Robust Pricing in Contextual Auctions. Operations Research 69, 1 (Jan. 2021),\n297–314. https://doi.org/10.1287/opre.2020.1991\n[30] Gurobi Optimization, LLC. 2022. Gurobi Optimizer Reference Manual. https://www.gurobi.\ncom",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_108",
    "file_path": "CSE_1.pdf"
  },
  "chunk-49ee7a4229cae88bca7a7bdbb6a932bd": {
    "tokens": 298,
    "content": "freedom (DOF) are χ2/DOF ≈ 0.89 and χ2/DOF ≈ 0.88 for the DESY5 and Pantheon+ data, respectively.\nFor the comparison to other dark energy models, we provide the best-fit points of ΛCDM, ωCDM, and\nω0ωaCDM [9, 10] in Table 2 and their χ2 difference to the ( ν)CKN models in Table 3. To compare between\nmodels with a different number of model parameters k, we use the Akaike information criterion (AIC)\nAIC = χ2\nmin + 2k . (2)\nThe results show that the CKN and νCKN models are preferred with respect to the ΛCDM model for both\ndatasets. In case of the νCKN model, the χ2 difference can be translated into a significance of 2 .63 σ and\n1.75 σ for the DESY5 and the Pantheon+ data, respectively. However, the ∆ χ2 and ∆AIC values show that\nthe ωCDM and ω0ωaCDM models provide an even better fit to the data. Only according to the ∆AIC values\nfor the combination with the Pantheon+ dataset, both, the CKN and νCKN models are slightly preferred with\nrespect to the ω0ωaCDM model.",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_4",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-2ed0be1f5f893f05a9a1125a75c53f3e": {
    "tokens": 170,
    "content": "0.32 0.34 0.36 0.38\n60\n65\n70\n75\n0.32 0.34 0.36 0.38\n130\n135\n140\n145\n150\n155\n160\n60 65 70 75\n130\n135\n140\n145\n150\n155\n160\nFigure 2: Correlations of H0–Ω0\nM (top left), H0–rd (top right), rd–Ω0\nM (bottom left) in the CKN model for the\nDESI BAO+Hubble+Pantheon+ DR1 (black dashed lines) and DESI BAO+Hubble+Pantheon+ DR2 (blue\narea) dataset at the 95 % and 68 % CL.\n5",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_10",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-dd578a75be84b25fcee6b2ccec16fff5": {
    "tokens": 382,
    "content": "|Θ|= 2 n= 5 n= 10n= 15m= 5 40/10056/10073/100m= 1033/10067/10066/100m= 1540/10062/10068/100\n|Θ|= 3 n= 5 n= 10n= 15m= 5 53/10085/10091/100m= 1057/10083/10085/100m= 1552/10079/10089/100\n|Θ|= 4 n= 5 n= 10n= 15m= 5 65/10090/10095/100m= 1070/10085/10093/100m= 1557/10089/10095/100\n|Θ|= 5 n= 5 n= 10n= 15m= 5 71/10094/10098/100m= 1079/10093/10099/100m= 1560/10092/10098/100\nTable 4: Probability of Assumption 1 being satisfied under 100 uniform random bayesian\nStackelberg game instances. To generate each instance, we sample every game parameter Ri,j and\nCθ\ni,j ∀i ∈ [m], j∈ [n], θ∈ Θ independently from the uniform distribution over [0 , 1]. The prior\ndistribution µ is uniform over Θ. Each row represents the number of leader actions, and the\ncolumn represents the number of follower actions.\nit is clear that a dynamic policy will always perform at least as well as any static policy in a\nStackelberg game, even in scenarios where learning is not feasible. However, a limitation of the",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_77",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0aa6aac5a0149ce6da883509c8e5cc6b": {
    "tokens": 337,
    "content": "Again, if we let H1, H2, ...,Hn ∈ G(m, m−1) be distributed according to ϕ, and let L ∈G(m, k)\nbe distributed according to ϕ∗ (satisfying the same conditions as ϕ), then we choose a random\nk-dimensional cone in TL. Note that H1, H2, ...,Hn and L are in general position with probability 1.\nThis random cone, denoted Ck\nn is precisely the polyhedral cone given by the following:\nP(Ck\nn ∈ B) =\nZ\nG(m,m−1)\nZ\nG(m,k)\n1\nC(n, k) ·\nX\nC∈Fd(Hn)\n1(C ∩ L ̸= {0})1 B(C)ϕ∗(dL)ϕn(d(Hn)) (15)\nCorollary 5 (Corollary to Corollary 4.2 in Hug and Schneider[35]). Let ϕ be a probability measure as\nindicated above, and let n ∈ N and H1, H2, ...,Hn ∈ G(m, m−1) be independent random hyperplanes\ndistributed according to ϕ. Then the probability that a random hyperplane, L, distributed according\nto ϕ∗ intersects with a random (ϕ, n)-Schl¨ afli coneSn is:\nE(Uk,ϕ∗(Sn)) = C(n, m− k)\n2C(n, m) (16)\n29",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_130",
    "file_path": "CSE_1.pdf"
  },
  "chunk-fca8d8b1ed4efc6124f41cafab00de83": {
    "tokens": 235,
    "content": "In many settings of interest, the leader may not know the follower’s payoff matrix C. To capture\nthe leader’s uncertainty about follower payoffs, we follow the literature and model the uncertainty\nas a random follower type θ ∈ Θ which is known privately to the follower while the leader only has\na prior distribution µ ∈ ∆|Θ| over the types. This prior distribution, µ, may be a result of either\nleader beliefs or may be due to observing a population of followers from which the prior can be\nlearned. We denote the payoff matrix of a θ-type follower as Cθ, and the best response for follower\ntype θ to any leader strategy x as j∗θ(x) = arg maxj∈[n] V θ(x, j). As a natural extension of SSE\nto this Bayesian setup, a leader with prior knowledge µ will play an x∗ to maximize her expected\nutility, formally, x∗ = arg maxx∈∆m\nP\nθ∈Θ µ(θ)Uθ(x, j∗θ(x)), known as the Bayesian Stackelberg\n6",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_42",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b878e8ab19fe81296b0d96fca8918556": {
    "tokens": 126,
    "content": "literature. Given the foundational nature of learning in strategic settings, particularly around\npricing, this work connects to a large body of existing work.\nWhile our setting is more restrictive than general mechanism design due to the limited ability for\nthe leader and follower to communicate, it is closely related to the literature on dynamic mechanism\ndesign. The most closely related work is by Balseiro et al . [8], who study the general dynamic\nmechanism design problem with a single principal (the leader) and a single agent (the follower).\nThe agent receives shocks to their utility function over time, drawn from an unknown distribution.\n3",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_28",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b6d8cc707bd4f176fd87e5a06283884b": {
    "tokens": 176,
    "content": "perform well on a set of standard Stackelberg games adapted for the repeated Bayesian setting.\nBoth heuristics approximate the optimal dynamic leader policy effectively and are significantly more\ncomputationally efficient, with the First- k rounds policy consistently outperforming the Markovian\npolicy in our experiments, while also being a polynomial-time algorithm for fixed k.\nTogether, these contributions suggest that in fully strategic settings beyond the pricing problem,\nlearning is likely both effective and feasible. Thus, our results imply that in fully strategic settings\noutside of dynamic pricing, it is unnecessary to weaken the strategic assumptions on the follower to\nachieve positive results regarding learning, contrary to the standard approach in the literature.\n1.2 Related Work\nIn this section, we discuss the relationship between this work and several distinct strands in the\nliterature. Given the foundational nature of learning in strategic settings, particularly around",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_27",
    "file_path": "CSE_1.pdf"
  },
  "chunk-395821a82cc56a4cf22b5ba46529d4d8": {
    "tokens": 117,
    "content": "[8] Santiago R. Balseiro, Anthony Kim, and Daniel Russo. 2021. On the Futility of Dynamics\nin Robust Mechanism Design. Operations Research 69, 6 (Nov. 2021), 1767–1783. https:\n//doi.org/10.1287/opre.2021.2122\n[9] David P Baron and David Besanko. 1984. Regulation and information in a continuing relation-\nship. Information Economics and policy 1, 3 (1984), 267–302.\n21",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_104",
    "file_path": "CSE_1.pdf"
  },
  "chunk-e6b197113ef81b7216f22de8228ab670": {
    "tokens": 236,
    "content": "\u0010\n1\n|Θ|\n\u0011\n≥ O\n\u0000\ne−|Θ|\u0001\n, the probability that Assumption 1 is not satisfied is O\n\u0010\n1\n|Θ|\n\u0011\n.\nB.3 Additional Simulation Results\nTable 7 shows the proportion of random games that permit effective learning when all game\nparameters for R and C are drawn IID from the standard normal distribution. The insights are\nidentical to Table 4.\nC Omitted Details in the Proof of Theorem 4\nProof. Proof of Theorem 4. The high-level proof idea is as follows. Given any RME {p∗\nθ,j, x∗\nθ,j}j∈[n],θ∈Θ,\nwe want to construct a dynamic policy (not necessarily the optimal DSE) that can “simulate” the\ngiven RME. Unfortunately, this construction has two main challenges that we have to overcome.\nFirst, the RME offers a menu of randomized strategies {p∗\nθ,j, x∗\nθ,j}j∈[n] for each type θ. Because of\n34",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_143",
    "file_path": "CSE_1.pdf"
  },
  "chunk-32e1e82baac8a32d88f7b6a9a0a58c2f": {
    "tokens": 148,
    "content": "where we sample games from a multivariate normal distribution (see Table 7 in Appendix B.3), we\nleave to future work the generality of this phenomenon.\nThe effect of the number of leader actions appears to be more ambiguous. Intuitively, it seems\nreasonable that as the number of leader actions increases, learning should be less likely to be effective\nsince the leader has a larger action space from which to construct optimal static strategies. However,\nthe simulation results in Table 4 are inconclusive as to the overall effect.\n4 Learning Versus Communication\nIn the preceding section, we demonstrated that learning can be effective for general games, especially\nwhen there are numerous possible types and sufficient rounds to facilitate learning. Furthermore,\n14",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_76",
    "file_path": "CSE_1.pdf"
  },
  "chunk-6dd8485fa21939d6b972d8f046a9ff78": {
    "tokens": 192,
    "content": "distribution of traffic over time through observing latency throughout the network.\nThe preceding examples, including the dynamic pricing problem, can all be formalized using the\nframework of Bayesian Stackelberg games [ 18]. Specifically, a standard Stackelberg game models a\ntwo-step sequential decision-making process between two agents, a leader and a follower. In the\ndynamic pricing problem, the leader would be the seller and the buyer would be the follower. The\nleader moves first in a Stackelberg game by committing to a randomized strategy, e.g. randomizing\nover prices for an item. Then the follower responds with a utility maximizing action, e.g. does\nor does not buy the item, after observing the leader’s strategy. However, the follower’s utility\ninformation, the valuation of the buyer in the dynamic pricing problem, may be private and unknown\nto the leader, and instead the leader may only know a distribution over possible follower types,",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_22",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0d9aae053b731d79f56916e0547c374e": {
    "tokens": 210,
    "content": "or investigate the possibility of learning in specific instances [9, 10, 19].\nIn this work, we assume that the leader can fully commit to their strategy, while several works\nin the dynamic mechanism design literature consider cases where the leader cannot commit. When\nthe leader cannot commit, it leads to the well-known ”ratchet effect” (see, for example, Freixas et al .\n[25], Laffont and Tirole [38]). In this scenario, the ability to learn is diminished due to the inability\nto credibly commit to respond appropriately to any revealed information. This is precisely what\nFreixas et al. [25], Laffont and Tirole [38] identify in a two-stage interaction between a principal and\nan agent, where the agent is incentivized to underproduce to avoid a demanding production schedule\nin future rounds. We are interested in the possibility of learning in settings with full strategic power,\nso we consider the ability to commit as a reasonable assumption. This assumption is also widely",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_32",
    "file_path": "CSE_1.pdf"
  },
  "chunk-dc141bdf3855f4bc54be169730f3a046": {
    "tokens": 203,
    "content": "to identify a subset of types to target for the dynamic policy. Starting from the BSE, the leader\nchooses to deviate to a strategy that improves relative to the BSE in the last round if and only if the\nagent responds consistently as the chosen subset. All types of agents are incentivized to respond\nwith their BSE best response in each round due to it being costly to emulate another type. We show\nthat given some assumptions, this policy strictly improves relative to the BSE.\nAgain, this constructed policy switches from a BSE for the whole group to a BSE for a subset\nof the types. In both cases, the commitment power does not increase the utility relative to purely\nlearning. Stated differently, the leader switches to an optimal static strategy after being sufficiently\nconvinced that the follower’s type belongs to a subset of the types, though she does commit to only\nexploiting this information in the final round which is essential to prevent other types pooling with",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_60",
    "file_path": "CSE_1.pdf"
  },
  "chunk-071d563afc1d6e356e25918b6c3e160b": {
    "tokens": 172,
    "content": "Definition 3 (Best Response Set). For a given leader strategy x, the set of best responses for a\nsub-group Θ′ ⊂ Θ is all follower actions that achieve maximal utility for some follower in subset Θ′,\ni.e.,\nBR(Θ′, x) =\nn\nj∗ ∈ [n]\n\f\f ∃θ ∈ Θ′ such that j∗ ∈ argmax\nj∈[n]\nV θ(x, j)\no\n.\nSimilarly, for a given type θ ∈ Θ and action j ∈ [n], we define the region of the leader strategy space\nfor which j is the best response for type θ as BRθ(j), i.e.\nBRθ(j) = {x ∈ ∆m | j ∈ BR(θ, x)}.\n11",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_62",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ded8bc52cef0c0c4559b8559154d7ca5": {
    "tokens": 312,
    "content": "\u0013\nin the Theorem.\nInstance with Ω(1/T) lower bound. We remark that the upper and lower bound of the\nutility comparison above is off by a factor of O(1/\n√\nT). Closing this gap is an interesting open\nquestion. To prove the Ω(1 /T) bound, we consider the following game instance\nR j0 j1\ni0 1 0\ni1 0 1\nC0 j0 j1\ni0 1 0\ni1 1 0\nC1 j0 j1\ni0 0 1\ni1 0 1\nin which the leader has a prior distribution (0 .5, 0.5) over the two follower types ( C0, C1). It is\nstraightforward that the RME is to offer xC0\n= (1, 0) with probability 1 for type C0; xC1\n= (0, 1)\nwith probability 1 for type C1. As a result, URME = 1. If the leader cannot offer a menu of strategies\nbut applies a dynamic policy instead, the optimal policy would be to use the first round to learn the\nfollower’s type based on his response (either j0 or j1) and play pure strategy i0 or i1 accordingly.\nThe optimal expected utility at the first round, however, is at most 0 .5. In this case, we have",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_154",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5b8d73029d578a51336e3a610ad699c2": {
    "tokens": 359,
    "content": "R j0 j1\ni0 0 0.4\ni1 0 0.5\ni2 0 0.6\nC0 j0 j1\ni0 0 0\ni1 0 -0.1\ni2 0 -0.2\nC1 j0 j1\ni0 0 0.1\ni1 0 0\ni2 0 -0.1\nC2 j0 j1\ni0 0 0.2\ni1 0 0.1\ni2 0 0\nFigure 5: Utility Matrices for a Dynamic Pricing Game. A pricing game with three buyer types,\nV = {0.4, 0.5, 0.6}, from a uniform distribution. The sellers actions are to set a price of\n{0.4, 0.5, 0.6}.\nE.1 Additional Results on Structured Games\nThe Dynamic Pricing Game. We compute the exact DSE using the MILP from Section 5 on\na dynamic pricing game, as described in Section 2.1.4, to experimentally verify the no learning\ntheorem. We examine a game with finite buyer value set V = {0.4, 0.5, 0.6} and a uniform value\ndistribution. It is easy to compute that the optimal single-round mechanism is to post the Myerson\nprice of 0.4, inducing expected revenue of 0 .4. This game can be written as a Stackelberg game,\nwith payoff matrices given by Figure 5. Here, R represents the seller’s utility matrix; C0, C1, C2",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_173",
    "file_path": "CSE_1.pdf"
  },
  "chunk-4eb34d4e34dc0efc5eeaff888a1401da": {
    "tokens": 237,
    "content": "dynamic policy results in a total leader utility of 10 3\n4, averaged to 5 3\n8 per round, which is strictly\nlarger than the static optimal leader utility of 5 1\n6.\nThe optimal dynamic policy also outperforms the leader’s static strategy with complete knowledge.\nSpecifically, suppose the leader can observe the follower’s type before the game and can play the\noptimal static strategy against each type (i.e. the SSE), which is x = (1, 0) against C0 and x = (1\n3, 2\n3)\nagainst C1. In this case, the leader obtains expected utility 5 against follower type C0 and 5 1\n3\nagainst C1, both of which are less than the averaged dynamic utility 5 3\n8. This is because dynamic\nstrategies, with commitment, are intrinsically more powerful than static strategies. Specifically,\nthe leader is able to induce type 1 to play j1 in the first round, when he would prefer to play j0,\nbecause the follower is rewarded in the second round with the more favorable leader strategy of",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_52",
    "file_path": "CSE_1.pdf"
  },
  "chunk-7dc09ba188866ef289243b7966d59be1": {
    "tokens": 595,
    "content": "xδ\nθ,j\nFor every follower type θ ∈ Θ and θ′ ∈ Θ such that θ′ ̸= θ, by definition\nX\nj\npδ\nθ,jV θ(xδ\nθ,j, j) ≥\nX\nj\npδ\nθ′,j max\nj′\nV θ(xδ\nθ′,j, j′) + δ,\nX\nj\np∗\nθ,jV θ(x∗\nθ,j, j) ≥\nX\nj\np∗\nθ′,j max\nj′\nV θ(x∗\nθ′,j, j′).\n(34)\nAs a result, we have for all θ′ ̸= θ\nX\nj\npθ,jV θ(xθ,j, j) =\nX\nj\n \n1 −\nr\nlog |Θ|\nT δ2\n!\np∗\nθ,jV θ(x∗\nθ,j, j) +\n r\nlog |Θ|\nT δ2\n!\npδ\nθ,jV θ(xδ\nθ,j, j)\n≥\nX\nj\nmax\nj′\n \n1 −\nr\nlog |Θ|\nT δ2\n!\np∗\nθ′,jV θ(x∗\nθ′,j, j′) +\nX\nj\nmax\nj′\nr\nlog |Θ|\nT δ2 pδ\nθ′,jV θ(xδ\nθ′,j, j′) + δ\nr\nlog |Θ|\nT δ2\n≥\nX\nj\nmax\nj′\n  \n1 −\nr\nlog |Θ|\nT δ2\n!\np∗\nθ′,jV θ(x∗\nθ′,j, j′) +\nr\nlog |Θ|\nT δ2 pδ\nθ′,jV θ(xδ\nθ′,j, j′)\n!\n+ δ\nr\nlog |Θ|\nT δ2\n=\nX\nj\nmax\nj′\npθ′,jV θ(xθ′,j, j′) +\nr\nlog |Θ|\nT .\nwhere the first inequality is by (34) and the second inequality is by merging two max’s into one\nmax, proving the constructed mechanism ⟨p, x⟩ is O(\nq\nlog |Θ|\nT )-strictly IC.\nNext, we write out the leader’s expected utility for every type θ\nX\nj\npθ,jU(xθ,j, j) =\nX\nj\n(1 −\nr\nlog |Θ|\nT δ2 )p∗\nθ,jU(x∗\nθ,j, j) + (\nr\nlog |Θ|\nT δ2 )pδ",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_149",
    "file_path": "CSE_1.pdf"
  },
  "chunk-f2aee25f8bdc2381d21abb0ad61126d0": {
    "tokens": 272,
    "content": "this sub-group of types. Therefore, commitment is essential to the learning strategy, but learning is\nsufficient to ensure a higher utility for the leader.\nWe will make use of the notion of a sub-group BSE, defined as follows:\nDefinition 2 (Sub-group BSE). Given a Bayesian Stackelberg game {R, Θ, {Cθ}θ∈Θ, µ}, con-\nsider the sub-group of types Θ′ ⊂ Θ and re-normalized distribution µ′(θ) = µ(θ)P\nθ′∈Θ′ µ(θ) for θ ∈\nΘ′. The sub-group BSE for Θ′, denoted BSE(Θ′), is the BSE for the Bayesian Stackelberg game\n{R, Θ′, {Cθ}θ∈Θ′, µ′}.\nNote that for a sub-group of size one, i.e. {θ} for some θ ∈ Θ, BSE({θ}) would correspond to\nthe SSE for type θ. Additionally, we will be concerned with both the sets of best responses for a\ngiven set of types assuming a certain leader strategy as well as the set of static leader strategies\nthat induce a certain best response for each type. We define the notation for these sets as follows:",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_61",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c35a298ae4ffbeb9c573e889792851ed": {
    "tokens": 296,
    "content": "[39] Niklas Lauffer, Mahsa Ghasemi, Abolfazl Hashemi, Yagiz Savas, and Ufuk Topcu. 2022. No-\nRegret Learning in Dynamic Stackelberg Games. arXiv preprint arXiv:2202.04786 (2022).\n[40] Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. 2009. Learning and approximating\nthe optimal strategy to commit to. In International symposium on algorithmic game theory .\nSpringer, 250–262.\n[41] Tao Li and Suresh P Sethi. 2017. A review of dynamic Stackelberg game models. Discrete &\nContinuous Dynamical Systems-B 22, 1 (2017), 125.\n[42] Vahab Mirrokni, Renato Paes Leme, Pingzhong Tang, and Song Zuo. 2020. Non-Clairvoyant\nDynamic Mechanism Design. Econometrica 88, 5 (2020), 1939–1963.\n[43] Mehryar Mohri and Andres Munoz. 2014. Optimal regret minimization in posted-price auctions\nwith strategic buyers. In Advances in Neural Information Processing Systems . 1871–1879.\n[44] Mehryar Mohri and Andres Munoz. 2015. Revenue optimization against strategic buyers. In",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_112",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5cbf66b0f4b962955618e68ed1a1c956": {
    "tokens": 84,
    "content": "Table 11: Additional experimental results on randomized games. Running time (columns 2-4 with\nthe unit: second) and average utility (columns 5-8) for random game instances with\nm = 3, n= 3, |Θ| = 3, k = 3, where “N/A” means the algorithm can not return a solution within 3\nhours.\n45",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_178",
    "file_path": "CSE_1.pdf"
  },
  "chunk-cebab85ac09811750cca68f04254465a": {
    "tokens": 211,
    "content": "where we show that it is indeed possible for an uninformed leader to learn effectively against a fully\ninformed follower in the majority of games.\n1.2.2 Learning Optimal Prices\nThe motivation for this work was the No Learning Theorem in dynamic pricing, and therefore the\nliterature on learning optimal prices is closely related. Given the breadth of this literature we will\nbe restricting our discussion to work on learning in strategic settings. Given that the No Learning\nTheorem places a strict impossibility for the fully strategic setting, work has generally been focused\non the relaxation of some aspect of the strategic capacity of the buyer. When the buyer discounts\nthe future at a greater rate than the seller, there are positive results [ 4, 5, 29, 43, 44], e.g. there\nexists a no-regret learning policy to learn the optimal price. Other work has focused on relaxing\nthe commitment assumption in pricing [ 36] or restricting the strategy space in some way [ 20, 21].",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_36",
    "file_path": "CSE_1.pdf"
  },
  "chunk-4aa44b4f5f3ca37bd19a4246568ec388": {
    "tokens": 361,
    "content": "jt ≤ 1 by the first two\nconstraints of (45b), as well as zt,θ\njt ≥ 1 by the last constraint of (45b). Hence, zt,θ\njt = 1, and the\nsecond constraint of (45c) will not add additional constraint on wt,θ\njt,i. By assumption wt,θ\njt,i ̸= xt\njt−1,i,\nthen we must have wt,θ\njt,i < xt\njt−1,i because of the first constraint of (45c). In this case, we can always\nconstruct another bw such that bwt,θ\njt,i = xt\njt−1,i. The new bw satisfies all the constraints, so it would\nstill be feasible but has a higher objective value (45) since Ri,j ∈ [0, 1], which contradicts with w is\npart of an optimal solution.As a result, we have shown that for any optimal solution x, y, a, z, w of\nMILP (45), we must have wt,θ\njt,i = xt\njt−1,i · Qt\nt′=1 yθ\nt′,jt′ . Thus, x, y, a form a feasible solution to MIP\n(47) with the same objective value, implying OP T(45) ≤ OP T(47).\nD.2 MILPs to Compute the DSE Approximately\nD.2.1 MILP to compute the Markovian policy.\nmaximize\nX\nt∈[T]\nX\nθ∈Θ\nh\nµ(θ) U(xt\njθ\nt−1\n, jθ\nt )\ni\n(53a)",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_171",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5c2b8bd3a5b3d41919cfb9686b3f5ae2": {
    "tokens": 385,
    "content": "the same objective function. It is obvious to see that x, y and a are still feasible to (47) since\nthe constraints of (47) is a subset of constraints of (45). The key step is to show these x, y and\na achieve the same objective value in (47). To show this, we prove that for any optimal solution\nx, y, a, z, w of (45), we must have wt,θ\njt,i = xt\njt−1,i · Qt\nt′=1 yθ\nt′,jt′ for any θ, t,and jt. We prove this is\ncorrect through a careful case analysis:\n(1) If there exists some ji ∈ (j1, ··· , jt) such that yθ\ni,ji = 0, then wt,θ\njt,i = xt\njt−1,i · Qt\nt′=1 yθ\nt′,jt′ = 0.\n(2) If yθ\ni,ji = 1 for all i = 1, ··· , t, we claim wt,θ\njt,i = xt\njt−1,i · Qt\nt′=1 yθ\nt′,jt′ = xt\njt−1,i. For the sake of\ncontradiction, let us assume wt,θ\njt,i ̸= xt\njt−1,i. Note that we must have wt,θ\njt,i ≤ xt\njt−1,i and wt,θ\njt,i ≤ zt,θ\njt\nby constraint (45c). Note that when yθ\ni,ji = 1 for i = 1, ··· , t, we have zt,θ\njt ≤ 1 by the first two\nconstraints of (45b), as well as zt,θ",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_170",
    "file_path": "CSE_1.pdf"
  },
  "chunk-1a5f78317d3eaea7a5a30c11e631c290": {
    "tokens": 230,
    "content": "2.1.4 Dynamic Pricing Games\nThe key motivation for this work is the No Learning Theorem for the dynamic pricing game with a\nsingle buyer with fixed valuation, i.e. the dynamic pricing problem . In this section, we formally\ndefine the dynamic pricing problem. In this game, a seller (she) repeatedly sells an item to the\nsame buyer (he) for T rounds. The buyer has a private value v ∈ R+ for the item, which is drawn\nfrom some prior distribution µ before the game starts and is fixed throughout the game. The seller\nknows the prior distribution µ and can post a price pt at each round t; the buyer responds with\njt ∈ {0, 1}, indicating accepting the price and buying the item ( jt = 1) or not. The buyers value is\njt · (v − pt) for round t. Before this game starts, the seller commits to a dynamic pricing policy π\nthat maps any buyer’s past responses to a price at the current round t, i.e. pt = π(j1, ··· , jt−1).",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_47",
    "file_path": "CSE_1.pdf"
  },
  "chunk-c643eeee60de8d81a9be40db0aad56f8": {
    "tokens": 243,
    "content": "game.\nFinally, Assumption 1 is a sufficient condition, not a necessary condition. For example, the\npricing game in Example 3 does not satisfy Assumption 1. Therefore, any estimation of the\nproportion of games for which Assumption 1 holds will be an upper bound on the true number of\ngames for which learning is effective. The proof of the theorem can be found in Appendix B.1.\nTheorem 2. Given a Bayesian Stackelberg game {R, Θ, {Cθ}θ∈Θ, µ} that satisfies assumption 1,\nthere exists a T∗ such that for all T ≥ T∗, the dynamic Bayesian Stackelberg game {R, Θ, {Cθ}θ∈Θ, µ, T}\nadmits a DSP, π, that learns effectively.\n3.2 Effective Learning in Random Bayesian Stackelberg Games\nWhile Theorem 2 indicates that Assumption 1 is sufficient to ensure that learning is effective, the\nprimary question is whether or not Assumption 1 is reasonable. In this section, we demonstrate\nthat for a randomly generated Bayesian Stackelberg game the probability that effective learning",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_65",
    "file_path": "CSE_1.pdf"
  },
  "chunk-a45ab5b8c4c687495a96bd193a3c14dc": {
    "tokens": 261,
    "content": "where we denote by Cθ\n·,j the column vector of utilities for the follower of type θ ∈ Θ when he\nplays action j. We will denote by G(m, k) the Grassmanian of k-dimensional linear subspaces\nof Rm. Therefore, Hθ\ni,j ∈ G(m, m− 1). Additionally, we will be working with the half spaces of\nleader strategies for which i is (weakly) preferred to j, i.e. Hθ,+\ni,j = {x ∈ Rm | x · (Cθ\n·,i − Cθ\n·,j) ≥ 0}.\nNote that for the leader strategies to be valid, we must restrict the sets to the intersection with\n∆m. However, for this section, we will generally not make that explicit. We will demonstrate the\nnon-existence of leader strategies that satisfy certain conditions, and clearly if a leader strategy\ndoes not exist in Rm, it does not exist in ∆ m ⊂ Rm. We will denote a random variable using a\ncalligraphic font, e.g. Hθ,+\ni,j .\nDefinition 4 (Random Bayesian Stackelberg Game). A random Bayesian Stackelberg Game is a",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_68",
    "file_path": "CSE_1.pdf"
  },
  "chunk-f25fe5624bebc1d4eefd0812a3e05d18": {
    "tokens": 178,
    "content": "in games with many possible follower types and “random” payoff matrices. We demonstrate this\nthrough an average case analysis for the space of dynamic Bayesian Stackelberg games. Specifically,\nwe define the concept of a random game and impose a condition on the distribution generating this\nrandom game. We then prove using a novel stochastic geometry based argument that the probability\nof the leader being able to learn effectively converges to one linearly with the number of follower\ntypes. This average case analysis highlights that the dynamic pricing problem is an anomaly within\nthe broader set of dynamic Bayesian Stackelberg games, suggesting that the insights drawn from\nthe No Learning Theorem do not generally apply in symmetric strategic settings. In the process\nof proving this claim, we develop a sufficient condition that guarantees effective learning by the\nleader, and we show that this condition holds with high probability. Despite the condition being",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_24",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0232f757ed8ca409e0599b5c26fd1bcf": {
    "tokens": 253,
    "content": "i=0\n\u0000n−1\ni\n\u0001 (20)\nProof. Proof. The corollary is a direct consequence of Corollary 5 with ϕ∗ = ϕ.\nLemma 2. Let N, k∈ N, with k <N+1\n2 . Then\nkX\ni=0\n\u0012N\ni\n\u0013\n≤\n\u0012N\nk\n\u0013 N − k + 1\nN − 2k + 1 (21)\nProof. Proof. Consider the sum:\nPk\ni=0\n\u0000N\ni\n\u0001\n\u0000N\nk\n\u0001 = 1 + k\nN − k + 1 + k(k − 1)\n(N − k + 1)(N − k + 2) + ··· (22)\n≤ 1 + k\nN − k + 1 +\n\u0012 k\nN − k + 1\n\u00132\n+ ··· (23)\n= N − k + 1\nN − 2k + 1 (24)\nNote that the final equality is due to k <N+1\n2 implying that k\nN−k+1 < 1.\n30",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_132",
    "file_path": "CSE_1.pdf"
  },
  "chunk-122a78fec4c6ed2929ad8a30e484d3c7": {
    "tokens": 209,
    "content": "i∈[t] yθ\ni,ji = t in this case, and thus P\ni∈[t] yθ\ni,ji −(t−1) = 1 = zt,θ\njt\nso the last constraint of (45b) is satisfied as well.\nTherefore, we have shown the constructed z, w together with x, y, a form a feasible solution to\nMILP (45) with the same optimal objective value as (47). This implies OP T(45) ≥ OP T(47),\nwhere OP Tdenotes the optimal solution value of the program.\nStep 2: OP T(45) ≤ OP T(47). We now prove the reverse direction, which is the more\ninteresting and non-trivial step. Specifically, suppose that we are given an optimal solution\nx, y, a, z, w of MILP (45). We show that these x, y and a form a feasible solution of (47) with\n42",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_169",
    "file_path": "CSE_1.pdf"
  },
  "chunk-353387dff5cdadbd31749378c6529779": {
    "tokens": 427,
    "content": "(this is where the non-degenerate inducibility gap is needed). A high-level idea is to construct a new\nrandomized menu that is both near-optimal and strictly IC by mixing the RME with the δ-strictly\nIC menu.\nLemma 3. For Stackelberg games with inducibility gap δ and T ≥ Ω(t2), there always exists a\nrandomized menu that is O\n\u0012q\nlog |Θ|\nT δ2\n\u0013\noptimal and O\n\u0012q\nlog |Θ|\nT\n\u0013\n-strictly IC.\nProof. Proof of Lemma 3 We prove the lemma by explicitly constructing a randomized menu ⟨p, x⟩\nand show that ⟨p, x⟩ is O\n\u0012q\nlog |Θ|\nT\n\u0013\n-strictly IC and achieves\n\u0012\n1 −\nq\nlog |Θ|\nT δ2\n\u0013\nURME. Specifically,\ndenote the RME as ⟨p∗, x∗⟩ and the δ-strictly IC randomized menu as ⟨pδ, xδ⟩, we construct the new\nrandomized menu ⟨p, x⟩ for all θ ∈ Θ and j ∈ [n] as follows\npθ,j =\n \n1 −\nr\nlog |Θ|\nT δ2\n!\np∗\nθ,j +\n r\nlog |Θ|\nT δ2\n!\npδ\nθ,j\nxθ,j =\n\u0012\n1 −\nq\nlog |Θ|\nTδ 2\n\u0013\np∗\nθ,j\npθ,j\nx∗\nθ,j +\n\u0012q\nlog |Θ|\nTδ 2\n\u0013\npδ\nθ,j\npθ,j\nxδ\nθ,j\nFor every follower type θ ∈ Θ and θ′ ∈ Θ such that θ′ ̸= θ, by definition\nX\nj\npδ\nθ,jV θ(xδ",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_148",
    "file_path": "CSE_1.pdf"
  },
  "chunk-491c87cb790880f6fad257c6c26d4166": {
    "tokens": 464,
    "content": "Runtime Average Utility\nFirst−k Markovian DSE First−k Markovian DSE RME\nT = 1 0.11 ± 0.13 0.1 ± 0.12 0.11 ± 0.13 0.9 ± .05 0.9 ± .05 0.9 ± .05 0.93 ± .04\nT = 2 2.21 ± 1.7 2.34 ± 1.4 2.19 ± 1.5 0.93 ± .03 0.93 ± .03 0.93 ± .03 0.93 ± .04\nT = 3 90 ± 13 8.4 ± 4.5 91 ± 13 0.95 ± .03 0.94 ± .03 0.95 ± .03 0.93 ± .04\nT = 4 3662 ± 865 73 ± 51 3683 ± 924 0.96 ± .02 0.95 ± .02 0.96 ± .02 0.94 ± .03\nT = 5 4645 ± 867 2181 ± 1929 N/A 0.96 ± .02 0.95 ± .02 N/A 0.94 ± .03\nT = 6 4614 ±564 12858\n±95791 N/A 0.96 ± .02 0.95 ± .02 N/A 0.94 ± .03\nT = 7 4480 ±831 N/A N/A 0.96 ± .02 N/A N/A 0.94 ± .03\nTable 6: Experimental Results on Randomized Games. Running time (columns 2-4 with the unit:\nsecond) and average utility (columns 5-8) for random game instances with m = 10, n= 5, |Θ| = 2,\nk = 3. “N/A” implies the algorithm did not return a solution within 3 hours.\nwithin two rounds in the battle of sexes game while in the SSG and game of chicken, UDSE did not",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_97",
    "file_path": "CSE_1.pdf"
  },
  "chunk-d61247a07c197fa6dc84363f9521ac3e": {
    "tokens": 249,
    "content": "Addendum: Fitting the DESI BAO Data with Dark\nEnergy Driven by the Cohen–Kaplan–Nelson Bound\nPatrick Adolf1∗, Martin Hirsch2†, Sara Krieg1‡, Heinrich P¨ as1§, Mustafa Tabet1¶\n1Fakult¨ at f¨ ur Physik, Technische Universit¨ at Dortmund, D-44221 Dortmund, Germany\n2Instituto de F` ısica Corpuscular (IFIC), Universidad de Valencia-CSIC,\nE-46980 Valencia, Spain\nApril 23, 2025\nAbstract\nMotivated by the recent Year-2 data release of the DESI collaboration, we update our results on time-\nvarying dark energy models driven by the Cohen–Kaplan–Nelson bound. The previously found preference of\ntime-dependent dark energy models compared to ΛCDM is further strengthend by the new data release. For\nour particular models, we find that this preference increases up to ≈ 2.6 σ depending on the used supernova\ndataset.\n1 Introduction\nIn this addendum, we update the results of our previous work [1] in the light of the recent Year-2 data release",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_0",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-fcc7268b1b0e3e46b41d66f0b6ea4329": {
    "tokens": 434,
    "content": "to be satisfied:\nX\nθ∈Θ′\nµ(θ)U(bx, j∗θ(bx)) >\nX\nθ∈Θ′\nµ(θ)U(x∗, j∗θ(x)) (8)\n(T − 1) · V θ(x∗, j∗θ(x∗)) + V θ(bx, j∗θ(bx)) ≥ (T − 1) · max\nj /∈BR(Θ′)\nV θ(x∗, j) + V θ(x∗, j∗θ(x∗)), ∀θ ∈ Θ′ (9)\nT · V θ(x∗, j∗θ(x∗)) ≥ (T − 1) · max\nj∈BR(Θ′)\nV θ(x∗, j) + V θ(bx, j∗θ(bx)), ∀θ ∈ Θ \\ Θ′ (10)\nConstraint 8 ensures that the leader improves her utility by switching from x∗ to bx in the final\nround of play. This constraint is satisfied by the assumption that BSE(Θ′) ̸= BSE.\nConstraint (9) ensures that all types in Θ ′ find it optimal to respond with j∗′\nfor rounds\n1, ..., T− 1. To see that this holds, first note that the following must be true by the definition of\nBR(Θ′)\nV θ(x∗, j∗θ(x)) > max\nj /∈BR(Θ′)\nV θ(x∗, j), ∀θ ∈ Θ′.\nThen, equation (9) will be satisfied if the following holds:\n(T − 1) · (V θ(x∗, j∗θ(x∗)) − max\nj /∈BR(Θ′)\nV θ(x∗, j)) ≥ 1\nSimilarly, constraint (10) ensures that all types not in Θ ′ do not find it optimal to deviate and",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_122",
    "file_path": "CSE_1.pdf"
  },
  "chunk-30f8809ddf01585f32152ad866154a46": {
    "tokens": 61,
    "content": "(2001), 213-224, [gr-qc/0009008].\n[10] E. V. Linder, Exploring the expansion history of the universe , Phys. Rev. Lett. 90 (2003) 091301,\n[astro-ph/0208512].\n8",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_15",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-3a70300ebe3df7c75a0e89dca4610c93": {
    "tokens": 247,
    "content": "a menu can be described by ⟨p, x⟩ = {pθ,j, xθ,j}j∈[n],θ∈Θ. After committing to this menu, the leader\nasks the follower to report his type θ. She then draws xθ,j with probability pθ,j and plays xθ,j,\nwhich will induce follower type θ to best respond with action j ∈ [n]. The computation of the\noptimal menu, which we coin the Randomized Menu Equilibrium ( RME), has been studied recently in\nStackelberg games [26] and contract design [15]. Gan et al . [26] shows that the RME can be computed\nin polynomial time.\nAs indicated, this is no longer a Bayesian Stackelberg game equilibrium since it requires both\ncommunication between the leader and the follower and that the leader must randomize over a\nmenu of (already randomized) mixed strategies. This implies the RME may not be applicable in many\nsettings since: (1) leader-follower communication is not possible in many domains, such as security\n[54]; (2) the randomized menu over mixed strategies may not be plausible as a commitment due to",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_80",
    "file_path": "CSE_1.pdf"
  },
  "chunk-2a6a791c507fda42463941055ac75208": {
    "tokens": 235,
    "content": "\u0000bk|Θ|mnbk\u0001\nnumber of continuous variables and |Θ|nbk integer variables, where bk = min(k + 1, T).\nThe running time of First-k only grows exponentially in k and the number of follower types\n|Θ|. The exponential complexity in |Θ| is unavoidable due to the inherent hardness of Bayesian\nStackelberg games even when T = 1 [18]. Unlike DSE and Markovian policies, however, the runtime\nof First-k is independent of T. As we demonstrate in the following experimental section, First-k\nappears to scale well with respect to T and achieves higher utility compared to Markovian with a\ndecreased running time.\n6 Experiments\nTo examine the efficacy of the DSE and the efficiency of our proposed algorithm, we perform several\nexperiments using Gurobi 9.5.1 solver on a machine with Ubuntu 20.04.5 LTS operating system, 2\n× 18 cores 3.0 GHz processors, and 256GB RAM.\nGame of Chicken. The game of chicken models situations in which two players desire a",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_92",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ae7378b0939f7dde0d19d5f433e25ed6": {
    "tokens": 186,
    "content": "by the repeated BSE is identical to the leader utility realized by the DSE.\nIt is important to note that in the pricing game, the seller does learn about the buyer’s type\nunder the optimal, static pricing, policy. Specifically, the seller will know whether or not v ≥ p∗.\nHowever, the seller cannot condition the pricing policy on the history of responses, so the seller\ndoes not learn effectively. Refer to Appendix A for a proof of the No Learning Theorem.\n3 Learning in Dynamic Bayesian Stackelberg Games\nDynamic pricing is a special case of a general dynamic Bayesian Stackelberg game with the seller\nas the leader and the buyer as the follower. Given the optimality of static pricing due to the No\nLearning Theorem, it becomes very natural to ask the following research question:\nIs it possible to learn effectively from a strategic follower in general dynamic Bayesian\nStackelberg games?",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_49",
    "file_path": "CSE_1.pdf"
  },
  "chunk-1ce0947327bda02b0d570cb2a354b3d3": {
    "tokens": 211,
    "content": "probability that Assumption 1 is not satisfied is O\n\u0010\n1\n|Θ|\n\u0011\n.\nSee Appendix B.2 for the detailed proof of Corollary 1. Corollary 1 demonstrates that for a fixed\nn and m, learning is likely to be effective for games with many follower types. We demonstrate this\nempirically in Table 4 where we sample random Bayesian Stackelberg games drawn uniformly at\nrandom with a uniform prior. Table 4 clearly shows that as the number of follower types increases,\nthe probability for a game to permit effective learning increases, as expected. However, it also\nsuggests that the likelihood of learning being effective should increase as the number of follower\nactions increase. It is unclear if this is an artifact of the way in which we sample the random games\nor if this is a more general phenomena. While we demonstrate that this pattern holds for the case\nwhere we sample games from a multivariate normal distribution (see Table 7 in Appendix B.3), we",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_75",
    "file_path": "CSE_1.pdf"
  },
  "chunk-752d5e475e7694837f08d4f4935d59c4": {
    "tokens": 373,
    "content": "for all θ and θ′ ̸= θ. When we approximate pθ with a (T − t)-uniform distribution pθ, there exists\nan approximate error ϵ =\nq\nlog 2(|Θ|+1)\n2(T−t) = O(\nq\nlog |Θ|\nT ) for all players’ utilities under the randomized\nstrategy {pθ,j, xθ,j}j∈[n]. As a result, for the ( T − t)-uniform distribution pθ, we have following\nobservations by Lemma 4 for all θ and θ′ ̸= θ:\nX\nj\npθ,jV θ(xθ,j, j) ≥\nX\nj\npθ,jV θ(xθ,j, j) − ϵ (38)\nand X\nj\npθ′,j max\nj′\nV θ(xθ′,j, j′) ≤\nX\nj\npθ′,j max\nj′\nV θ(xθ′,j, j′) + ϵ (39)\nTherefore, we can combine (37), (38), and (39), where ϵ = O\n\u0012q\nlog |Θ|\nT\n\u0013\n, to get\nX\nj\npθ,jV θ(xθ,j, j) ≥\nX\nj\npθ,jV θ(xθ,j, j) − O\n r\nlog |Θ|\nT\n!\n(40)\nand\nX\nj\npθ′,j max\nj′\nV θ(xθ′,j, j′) ≤\nX\nj\npθ,jV θ(xθ,j, j) − O\n r\nlog |Θ|\nT\n!\n(41)\n37",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_152",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ae2c127b5b535af365551cb83fe8cd96": {
    "tokens": 366,
    "content": "with payoff matrices given by Figure 5. Here, R represents the seller’s utility matrix; C0, C1, C2\nrepresent the buyer’s utility matrix when v = 0.4, v= 0.5, v= 0.6 accordingly; i0, i1, i2 represent\nthe seller sets a posted price 0 .4, 0.5, 0.6; j0, j1 corresponds to the buyer rejecting or purchasing the\nitem. We run experiments for T = 1, ...,10, and observe UDSE = 0.4T, as expected. That is, the\naverage optimal dynamic pricing utility is indeed the same as the optimal static revenue.\nStackelberg security game. Additionally, we consider the case where the leader has one more\ntarget that needs to be protected (i.e. more actions for both the leader and the follower). Specifically,\nconsider the following utility matrices for both agents.\nR t0 t1 t2\nt0 1 0 0\nt1 0 1 0\nt2 0 0 1\nC0 t0 t1 t2\nt0 0 0.5 0.5\nt1 1 0 0.5\nt2 1 0.5 0\nC1 t0 t1 t2\nt0 0 1 0.5\nt1 0.5 0 0.5\nt2 0.5 1 0\nC2 t0 t1 t2\nt0 0 0.5 1\nt1 0.5 0 1\nt2 0.5 0.5 0",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_174",
    "file_path": "CSE_1.pdf"
  },
  "chunk-14834d6e4479d985c941c094112e8b8c": {
    "tokens": 61,
    "content": "those discussed in the main paper. We continue to compare the runtime and average leader utility\nfor solving the optimal Markovian policy and the First-k policy, against solving the DSE. These\nadditional experiments lead to the same conclusion as in the main paper, reinforcing our previous\nfindings.\n44",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_176",
    "file_path": "CSE_1.pdf"
  },
  "chunk-074571787b552c02436a75ecf2425b25": {
    "tokens": 296,
    "content": "[54] Arunesh Sinha, Fei Fang, Bo An, Christopher Kiekintveld, and Milind Tambe. 2018. Stackelberg\nsecurity games: Looking beyond a decade of success. IJCAI.\n[55] Aleksandrs Slivkins et al. 2019. Introduction to multi-armed bandits. Foundations and Trends®\nin Machine Learning 12, 1-2 (2019), 1–286.\n[56] Heinrich von Stackelberg. 1934. Marktform und gleichgewicht. (1934).\n[57] Arsenii Vanunts and Alexey Drutsa. 2019. Optimal pricing in repeated posted-price auctions\nwith different patience of the seller and the buyer. Advances in Neural Information Processing\nSystems 32 (2019).\n[58] Hal R Varian. 2009. Online ad auctions. American Economic Review 99, 2 (2009), 430–34.\n[59] William Vickrey. 1961. Counterspeculation, auctions, and competitive sealed tenders. The\nJournal of finance 16, 1 (1961), 8–37.\n[60] Yevgeniy Vorobeychik and Satinder Singh. 2012. Computing stackelberg equilibria in discounted\nstochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 26.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_115",
    "file_path": "CSE_1.pdf"
  },
  "chunk-3f6adb6aa82daa51c0cc82a4c7cde778": {
    "tokens": 283,
    "content": "the random hyperplanes {Hθ\nj,j′}θ∈Θ are in general position with probability 1 [ 35]. A set of k-\ndimensional hyperplanes H1, H2, ..., Hn ∈ G(m, m− 1) are in general position if for any k ≤ m,\nthe hyperplanes have an intersection of dimension m − k. Another way to state this is to consider\nthe unit normal vector to the hyperplane H⊥\ni = {x ∈ Rm | x · y = 0 ∀ y ∈ Hi ∧ ||x|| = 1 }.\nThen H1, H2, ..., Hn ∈ G(m, m− 1) are in general position if for any subset of size m or less of\nH⊥\n1 , H⊥\n2 , ..., H⊥\nn ∈ Rm the subset is linearly independent.\nAssumption 2 holds for natural distributions over the set of payoff matrices. For example,\nany distribution f such that the columns of the follower’s payoff matrix Cθ\n·,j are independent and\nidentically distributed for all θ ∈ Θ and j ∈ [n] satisfies Assumption 2, assuming that the distribution\nover columns does not assign positive measure to a linear subspace of Rm. The second condition in",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_71",
    "file_path": "CSE_1.pdf"
  },
  "chunk-69dcb0bf35eb83941cc44b900d07907e": {
    "tokens": 408,
    "content": "Plugging this jT into (48), we must have M(T − P\nt yθ\nt,jt) ≥ M as a very large constant, which\nmakes the right-hand-side inequality void for any jT ∈ [n]T ̸= j∗θ\nT . Thus, the only useful constraint\nis 0 ≤ (aθ − P\nt∈[T]\nP\ni∈[m] xt\njt−1,iCθ\ni,jt) for any jT ̸= j∗θ\nT , which implies\nX\nt∈[T]\nX\ni∈[m]\nxt\njt−1,iCθ\ni,jt ≤ aθ, ∀θ, jT ̸= j∗θ\nT . (50)\nIt is now clear to see that j∗θ\nT is the optimal response for type θ by combining (50) and (49).\nReformulating the objective function in Program (2). We claim that for any feasible\nvariable x, y and a, the objective of Program (2) equals to the following expression:\nX\nθ\nµ(θ)\nX\nt∈[T],i∈[m],jt∈[n]t\nRi,jt xt\njt−1,i\ntY\nt′=1\nyθ\nt′,jt′ (51)\nThe objective above enumerates all possible path histories jt ∈ [n]t for any t ∈ [T]. However, the\nproduct Qt\nt′=1 yθ\nt′,jt′ is only non-zero when all {yθ\nt′,jt′ }t\nt′=1 are non-zero, i.e. when {jt′}t\nt′=1 is on the\noptimal follower response path for follower type θ. This guarantees that Objective (51) only counts",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_162",
    "file_path": "CSE_1.pdf"
  },
  "chunk-15cf7781a0902e676fd81d77eaeb44ea": {
    "tokens": 333,
    "content": "not assign a positive measure to a linear subspace. Given these preliminaries, we can now state our\nmain technical result.\nTheorem 3. For a fixed n and m and a randomly generated Bayesian Stackelberg game\n{R, Θ, {Cθ}θ∈Θ, µ, f} that satisfies Assumption 2, |Θ| −\nl\nlog3\n4\n1\n|Θ|\nm\n> 0, and |Θ|\n2 ≥ m − 2, then\nP\n\n [\ni∈[n]\n \\\nθ∈Θ\nBRθ(i)\n!\n̸= ∅\n\n ≤ O\n\u0012 1\n|Θ|\n\u0013\nPlease see Appendix B.2 for the full proof. Theorem 3 ensures that the second condition\nof Assumption 1 is satisfied with high probability; there exists a subgroup Θ ′ ⊂ Θ such that\nBR(Θ′, x∗) ∩ BR(Θ \\ Θ′, x∗) = ∅. Given that, for all i ∈ [n], the probability that there exists any\nx ∈ ∆m such that i ∈ BR({θ}, x) for all θ ∈ Θ is converging to zero as Θ increases implies that for\nx∗, the probability that the subgroup Θ ′ exists converges to one.\nThe final step is to ensure that for this subgroup the leader’s optimal strategy,BSE(Θ′) ̸= x∗. The",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_73",
    "file_path": "CSE_1.pdf"
  },
  "chunk-39c4983b0254f7dfce6577bb3a3ebf2c": {
    "tokens": 124,
    "content": "k = 1\n2\n\u0012\n− ((q + 1)(1 − m) + 4− 2m) +\np\nm2(q2 + 2q + 1) +m(−2q2 − 4q − 6) + q2 + 2q + 9\n\u0013\n= 1\n2\n\u0012\n((q + 1)(m + 1) − 4 + 2m) +\np\n(m − 1)2(q + 1)2 − 4m + 8\n\u0013\n31",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_135",
    "file_path": "CSE_1.pdf"
  },
  "chunk-f16548d340751dbd1ec19108571fe7b8": {
    "tokens": 191,
    "content": "and if the buyer does not buy, she offers the optimal price for the middle type.\n3.1 Sufficient Condition for Learning in Dynamic Bayesian Stackelberg Games\nIn the preceding examples, we have demonstrated that learning can be effective in dynamic Bayesian\nStackelberg games. However, the question remains, “Is learning generally effective for dynamic\nBayesian Stackelberg games?” In this section and the next, we formally show that, yes, learning is\ngenerally effective for dynamic Bayesian Stackelberg games in contrast to the No Learning Theorem.\nIn order to accomplish this, we characterize a sufficient condition to ensure that learning is effective.\nInterestingly, the proof, see Appendix B.1, demonstrating that the condition given in Assump-\ntion 1 is indeed sufficient is constructive, and it purely relies on learning, not commitment, to\nimprove relative to the optimal static policy. Specifically, the strategy that we will employ will be",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_59",
    "file_path": "CSE_1.pdf"
  },
  "chunk-313dc79925a9461af6c234c2f773476e": {
    "tokens": 191,
    "content": "policy π for T rounds, proving the proposition.\nAs a result, we have Mπ as a feasible mechanism for any dynamic pricing policy π and the\nseller’s revenue must be bounded by the Myerson revenue [45]:\n1\nT\nX\nt∈[T]\nj∗\nt (v)π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\n≤ MyersonRev, ∀π\nNote that P\nt∈[T] j∗\nt (v)π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\nis exactly the seller’s total revenue in T rounds when\nshe uses dynamic pricing policy π. Hence, we have shown that any dynamic pricing policy’s revenue\nis upper bounded by the revenue of running constant Myerson price for T rounds.\n26",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_120",
    "file_path": "CSE_1.pdf"
  },
  "chunk-83d001d7395be790d94cf13e474b5f8f": {
    "tokens": 92,
    "content": "θ µ(θ) P\nt,jt,i Ri,jt xt\njt−1,i\nQt\nt′=1 yθ\nt′,jt′ .\n2We omit the feasible regions of variables (e.g. t ∈ [T], i∈ [m], jT ∈ [n]T , θ∈ Θ) in some programs for the sake of\nspace when they are clear from the context.\n39",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_158",
    "file_path": "CSE_1.pdf"
  },
  "chunk-adef43b4875ffcbcf03b03391ca22eb8": {
    "tokens": 245,
    "content": "example, we show that learning can be arbitrarily more powerful than communication. However, we\ndo not have an equivalent result to Corollary 1 for the relative performance of the DSE versus the\nRME. Is it also true that for random games the DSE achieves strictly higher leader utility compared\nto the RME? Based on our experimental results, we believe this is very likely, but proving it remains\nan open question.\nReferences\n[1] Gagan Aggarwal, Ashish Goel, and Rajeev Motwani. 2006. Truthful auctions for pricing search\nkeywords. In Proceedings of the 7th ACM Conference on Electronic Commerce . 1–7.\n[2] Tal Alon, Paul D¨ utting, and Inbal Talgam-Cohen. 2021. Contracts with Private Cost per\nUnit-of-Effort. In Proceedings of the 22nd ACM Conference on Economics and Computation .\n52–69.\n[3] Ingo Alth¨ ofer. 1994. On sparse approximations to randomized strategies and convex combina-\ntions. Linear Algebra Appl. 199 (1994), 339–355.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_102",
    "file_path": "CSE_1.pdf"
  },
  "chunk-1536087932fb8728f33f3f496235a7ee": {
    "tokens": 279,
    "content": "so we restrict our attention to the static RME in this work.\nLet UDSE and URME denote the leader’s utility at DSE and RME. Our result on the comparison\nbetween UDSE and URME hinges on a non-degeneracy assumption pertaining to a notion coined the\ninducibility gap, denoted by δ, a concept adopted in previous work [27, 62].\nDefinition 5 (Inducibility Gap). The inducibility gap of a Stackelberg game with follower types\nΘ is the largest δ such that there exists an IC randomized menu {xθ,j, pθ,j}θ∈Θ,j∈[n] where for any\nfollower type θ: P\nj pθ,jV θ(xθ,j, j) ≥ P\nj pθ′,j maxj′ V θ(xθ′,j, j′) + δ, ∀θ′ ̸= θ.\nA positive inducibility gap simply means that every follower type θ ∈ Θ can be strictly\nincentivized to report truthfully by some randomized menu. This is a non-degeneracy assumption\nsince any Bayesian Stackelberg game trivially has δ ≥ 0 because playing any fixed mixed strategy x,\nirrespective of follower types, already (weakly) incentivizes truthful reports from every follower type.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_83",
    "file_path": "CSE_1.pdf"
  },
  "chunk-55e2d6df787e19eb93a0187055bb125b": {
    "tokens": 199,
    "content": "Stackelberg game, even in scenarios where learning is not feasible. However, a limitation of the\nBayesian Stackelberg framework is that it does not allow for direct communication between the\nleader and the follower beyond the leader committing to a strategy. For example, the leader cannot\nexplicitly ask the follower to report their type, as is typically allowed in broader mechanism design\nsettings. This raises a natural question: is a dynamic strategy simply a substitute for directly\neliciting the follower’s type? In other words, is communication between the leader and follower\ninherently more powerful than learning through repeated interactions?\nAlthough the No Learning Theorem holds for the pricing game even when the leader and follower\ncan communicate, indicating that learning in a dynamic setting generally cannot yield higher utility\nthan direct communication with a static policy (e.g., in pricing games), we demonstrate in this section\nthat the gap is minimal—up to an O\n\u0010\n1√\nT\n\u0011",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_78",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0e701cfcfa503cff69095549fa51adc1": {
    "tokens": 221,
    "content": "and develop heuristic algorithms to approximate the optimal dynamic policy more efficiently.\nThrough simulations, we compare the efficiency and runtime of these algorithms against static\npolicies.\n1 Introduction\nOptimal pricing given unknown demand is a well-studied problem that comes up in many settings,\nsuch as airline ticket pricing, ride-sharing platforms, and online retail, where pricing strategies are\ncrucial for maximizing revenue and efficiency [ 1, 58, 59]. A natural extension of the problem is to\nassume that there is an opportunity for a seller to learn the demand through repeated interactions\nusing dynamic pricing. However, even in the simplest setting, a single buyer with a fixed valuation\n∗This work is supported by Army Research Office Award W911NF-23-1-0030, ONR Award N00014-23-1-2802 and\nNSF Award CCF-2303372.\n†Work done when the author is at UVA CS.\n1\narXiv:2504.15568v1  [cs.GT]  22 Apr 2025",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_18",
    "file_path": "CSE_1.pdf"
  },
  "chunk-ac0df7fdf73160c28698a6fb6286d6c9": {
    "tokens": 282,
    "content": "Stackelberg game along with the optimal solution concept.\n2.1 Stackelberg Games\nWe consider a Stackelberg game played by two players, who are referred to as the leader (she)\nand the follower (he) . Each player has a finite action set, and we denote the leader’s action set\nas [m] = {1, ··· m} and the follower’s action set as [ n] = {1, ··· , n}. The leader’s utilities are\ndescribed by a matrix R ∈ Rm×n, in which Ri,j is the leader utility when she plays action i ∈ [m]\nand the follower responds with action j ∈ [n]. Similarly, the follower’s utility matrix is denoted\nby C ∈ Rm×n. We denote the Stackelberg game as {R, C}. Without loss of generality, we assume\nRi,j, Ci,j ∈ [0, 1], for all i, jin all statements of theorems. However, for example, we may allow\nRi,j, Ci,j ∈ R for the sake of expositional clarity.\nIn a single round (i.e. static) Stackelberg game [ 56], the leader moves first by committing to\na mixed strategy x ∈ ∆m, where ∆m = {x : P",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_40",
    "file_path": "CSE_1.pdf"
  },
  "chunk-f928e83ca6e9ea47109e2c91fa7b8fa3": {
    "tokens": 197,
    "content": "different direction, Deng et al . [22] studies dynamic policy design in a repeated setting when the\nfollower’s payoff information is public but instead of best responding the follower follows a no-regret\nlearning algorithm to interact with the leader. While this can be construed as a certain kind of\nlimited strategic behavior, it is very different than the fully strategic setting we are studying.\nIn [6], they consider the case of a “meta game” between two players who play an unknown, but\ndrawn from a known distribution, Stackelberg game. They examine cases under which the players,\nwho may be differentially informed, deploy a multi-round strategy as an action in the meta-game.\nThey show, consistent with the No Learning Theorem, that strategic interactions between these\nuninformed players cannot in all cases learn, and exploit the information learned, about the game in\norder to achieve their optimal Stackelberg strategy. In contrast, we conduct an average case analysis",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_35",
    "file_path": "CSE_1.pdf"
  },
  "chunk-694a053dc51cb40fb447ba0832926877": {
    "tokens": 178,
    "content": "-1 0 1 2 3\n0.28\n0.30\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\n-1 0 1 2 3\n60\n65\n70\n75\n-1 0 1 2 3\n130\n135\n140\n145\n150\n155\n160\nFigure 4: Correlations of ν–Ω0\nM (top left), ν–H0 (top right), ν–rd (bottom left) in the νCKN model for the\nDESI BAO+Hubble+Pantheon+ DR1 (black dashed lines) and DESI BAO+Hubble+Pantheon+ DR2 (blue\narea) dataset at the 95 % and 68 % CL.\n7",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_12",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-09ff8648da9df0e1d6ecff86f11f92b3": {
    "tokens": 260,
    "content": "R j0 j1\ni0 1 0\ni1 0 1\nC0 j0 j1\ni0 0.5 0\ni1 1 0\nC1 j0 j1\ni0 0 1\ni1 0 0.5\nTable 5: Utility Matrices for Example 4. A Bayesian Stackelberg game instance with two actions\nfor the leader and follower. The leader’s utility matrix is the first subtable R; the follower has two\npossible types, with utility matrix C0, C1 respectively, each having equal probability 0.5.\ntheoretic “optimality benchmark” for us to compare against. The main result of this section shows\nthat the DSE is guaranteed to achieve nearly identical utility, up to a small O\n\u0010\n1√\nT\n\u0011\ndiscrepancy.\nNote that we could, alternatively, consider the dynamic RME, i.e. the menu of dynamic policies,\nwhich would, by the revelation principle, be the strongest possible dynamic policy. However, we are\nconcerned with identifying and comparing to the strongest static policy that allows communication,\nso we restrict our attention to the static RME in this work.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_82",
    "file_path": "CSE_1.pdf"
  },
  "chunk-36719f3e6493e868b6cc84b4fbd7b2c4": {
    "tokens": 398,
    "content": "θ, t and (j1, ··· , jt), we consider the following two possible cases:\n• If there exists some ji ∈ (j1, ··· , jt) such that yθ\ni,ji = 0, then we have zt,θ\nji = 0 by construction.\nThen, we see by induction of zt′,θ\njt′ ≤ zt′,θ\njt′−1\nfor t′ ∈ [i + 1, ··· , t] that zt,θ\njt = 0 and consequently\nwt,θ\njt,i = 0 must hold as well. As a result, it is obvious that the first two constraints of (45b)\nand (45c) are satisfied. To see why the last constraint of (45b) holds, note that as long as at\nleast one of the ji ∈ (j1, ··· , jt) satisfy yθ\ni,ji = 0, then at least one of yθ\nt,jt and zt,θ\njt−1 must be 0\nas well, meaning yθ\ni,jt + zt,θ\njt−1 ≤ 0 = zt,θ\njt .\n• If yθ\ni,ji = 1 for all i = 1, ··· , t, then we have zt,θ\njt = 1 and wt,θ\njt,i = xt\njt−1,i by construction. It\nis also straightforward to see that constraint the first two constraints of (45b) and (45c) are\nsatisfied. Moreover, we have P\ni∈[t] yθ\ni,ji = t in this case, and thus P\ni∈[t] yθ\ni,ji −(t−1) = 1 = zt,θ\njt",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_168",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0a3cbe43ae448e98b4237809329a4142": {
    "tokens": 320,
    "content": "Acknowledgements\nP.A. is supported by the Studienstiftung des deutschen Volkes . M.H. acknowledges support by grants PID2020-\n113775GB-I00 (AEI/10.13039/ 501100011033) and CIPROM/2021/054 (Generalitat Valenciana).\nReferences\n[1] P. Adolf, M. Hirsch, S. Krieg, H. P¨ as and M. Tabet, “Fitting the DESI BAO data with dark energy\ndriven by the Cohen-Kaplan-Nelson bound,” JCAP 08 (2024), 048 doi:10.1088/1475-7516/2024/08/048\n[arXiv:2406.09964 [astro-ph.CO]].\n[2] M. Abdul Karim et al. [DESI], “DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\nCosmological Constraints,” [arXiv:2503.14738 [astro-ph.CO]].\n[3] A. G. Cohen, D. B. Kaplan and A. E. Nelson, Effective field theory, black holes, and the cosmological\nconstant, Phys. Rev. Lett. 82 (1999) 4971–4974, [hep-th/9803132].\n[4] D. Brout et al., The Pantheon+ Analysis: Cosmological Constraints , Astrophys. J. 938 (2022) 110,\n[2202.04077].",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_13",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-f2ca0daf620def2754ba3e686bf5edfe": {
    "tokens": 177,
    "content": "-1 0 1 2 3\n0.28\n0.30\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\n-1 0 1 2 3\n60\n65\n70\n75\n-1 0 1 2 3\n130\n135\n140\n145\n150\n155\n160\nFigure 3: Correlations of ν–Ω0\nM (top left), ν–H0 (top right), ν–rd (bottom left) in the νCKN model for\nthe DESI BAO+Hubble+DESY5 DR1 (black dashed lines) and DESI BAO+Hubble+DESY5 DR2 (blue area)\ndataset at the 95 % and 68 % CL.\n6",
    "chunk_order_index": 0,
    "full_doc_id": "astro_physics_1.pdf_11",
    "file_path": "astro_physics_1.pdf"
  },
  "chunk-676532c1c6f303986d4b1b45673a676d": {
    "tokens": 125,
    "content": "to the leader, and instead the leader may only know a distribution over possible follower types,\nhence the Bayesian Stackelberg game. In repeated settings there is an opportunity to learn from\nthe behavior of the follower. In this general framework, given the strong strategic position of the\nfollower, can the leader effectively exploit this information, in contrast to the No Learning Theorem\nfor the dynamic pricing problem?\n1.1 Contributions\nThis paper has three main contributions. Our first contribution is to show that learning and exploiting\nthe learned information—referred to as effective learning—is likely to be achievable, particularly\n2",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_23",
    "file_path": "CSE_1.pdf"
  },
  "chunk-91fed979424d4c952f211f2d2862784d": {
    "tokens": 218,
    "content": "Is Learning Effective in Dynamic Strategic Interactions? Evidence\nfrom Stackelberg Games∗\nMichael Albert\nUniversity of Virginia\nalbertm@darden.virginia.edu\nQuinlan Dawkins†\nAdvanced Micro Devices\nqdawkins@amd.com\nMinbiao Han\nUniversity of Chicago\nminbiaohan@uchicago.edu\nHaifeng Xu\nUniversity of Chicago\nhaifengxu@uchicago.edu\nAbstract\nIn many settings of interest, a policy is set by one party, the leader, in order to influence\nthe action of another party, the follower, where the follower’s response is determined by some\nprivate information. A natural question to ask is, can the leader improve their strategy by\nlearning about the unknown follower through repeated interactions? A well known folk theorem\nfrom dynamic pricing, a special case of this leader-follower setting, would suggest that the\nleader cannot learn effectively from the follower when the follower is fully strategic, leading\nto a large literature on learning in strategic settings that relies on limiting the strategic space",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_16",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b6f13d62e6b82ecb11dfc8ce5ad685ca": {
    "tokens": 455,
    "content": "Lemma 5. Program (2) is equivalent to the following MIP (47).\nmaximize\nX\nθ\nµ(θ)\nX\nt,jt,i\nRi,jt xt\njt−1,i\ntY\nt′=1\nyθ\nt′,jt′ (47a)\nsubject to\n0 ≤ aθ −\nX\nt,i\nxt\njt−1,iCθ\ni,jt ≤ M\n\nT −\nX\nt∈[T]\nyθ\nt,jt\n\n, ∀jT ∈ [n]T , θ∈ Θ, (47b)\nX\ni\nxt\njt−1,i = 1, ∀jt−1 ∈ [n]t−1, t∈ [T], (47c)\nxt\njt−1,i ∈ [0, 1], ∀i ∈ [m], jt−1 ∈ [n]t−1, t∈ [T], (47d)\nX\nj\nyθ\nt,j = 1, ∀t ∈ [T], θ∈ Θ, (47e)\nyθ\nt,j ∈ {0, 1}, ∀j ∈ [n], t∈ [T], θ∈ Θ. (47f)\nwhere x, y, a are decision variables and M is a sufficiently large constant.\nProof. Proof of Lemma 5. Reformulating follower’s incentive constraints in Program (2).\nWe argue that under our new variable yθ ∈ {0, 1}T×n, the following constraint is equivalent to the\nfirst set of constraints in Program (2) and thus correctly capture the follower’s dynamic optimal\nresponses:\n0 ≤ aθ −\nX\nt,i\nxt\njt−1,iCθ\ni,jt ≤ M(T −\nTX\nt=1\nyθ\nt,jt), ∀jT , θ, (48)\nwhere M is a very large constant, a ∈ Rk is a set of newly introduced decision variables.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_159",
    "file_path": "CSE_1.pdf"
  },
  "chunk-de69c82a43da5df72e7d3a2e2879c108": {
    "tokens": 218,
    "content": "because the follower is rewarded in the second round with the more favorable leader strategy of\nx2 = (1/2, 1/2). Stated another way, even if the leader knew the followers type, a dynamic policy\nwould outperform the optimal static policy due to commitment. In this example, both learning and\ncommitment contribute to the increase in the leader’s utility.\nWhile Example 1 demonstrates that learning can be effective in a Bayesian Stackelberg game,\nthis is certainly not guaranteed. We can reformulate the dynamic pricing game as a Bayesian\nStackelberg game, as Example 2 illustrates in the following, and then the No Learning Theorem\nimplies that no dynamic policy can outperform the optimal static policy.\nExample 2 (Pricing as a Stackelberg Game) . Consider the standard pricing problem where the\nseller sells a single item to a buyer whose value v over the item is drawn from a distribution which,\nas an example here, is the uniform distribution over set V = {8, 35, 96}. The seller will set prices",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_53",
    "file_path": "CSE_1.pdf"
  },
  "chunk-6a51938f1369ad836e42a15f6082337d": {
    "tokens": 181,
    "content": "at a possible value from set V . This game is then equivalent to a Bayesian Stackelberg game with\nutility matrices described in Table 2. It is easy to compute that the optimal Myerson price, and\ntherefore the optimal static policy, in this setting is 96, which induces expected seller revenue 32.\nIt can be verified using the approach we propose in Section 5 that the optimal dynamic seller\npolicy is to set the Myerson price 96 at every round, which is the optimal static price and is\nconsistent with the No Learning Theorem for the pricing game. However, we can modify the pricing\ngame slightly so that the seller’s (leader’s) action space does not include prices that correspond to\nthe buyer’s (follower’s) set of valuations. This small modification to the pricing game leads to a\nsignificantly different optimal dynamic strategy.\n9",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_54",
    "file_path": "CSE_1.pdf"
  },
  "chunk-2504f2b8fe5658de14cf5d175a7b3ea9": {
    "tokens": 160,
    "content": "so we consider the ability to commit as a reasonable assumption. This assumption is also widely\nused in the literature on dynamic principal-agent problems in both mechanism design [ 37, 48] and\nStackelberg games [39, 41].\n1.2.1 Learning in Stackelberg Games\nIn addition to the broader literature on dynamic mechanism design, there has been significant interest\nin learning in the narrower space of Stackelberg games. When the follower’s payoff information is\nunknown, Balcan et al. [7], Haghtalab et al. [32], Han et al . [33], Letchford et al. [40], Peng et al.\n[50] propose various policies that allow a leader to learn the follower’s utility by observing the\n4",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_33",
    "file_path": "CSE_1.pdf"
  },
  "chunk-82cae28334d2f028d89691a00cb3e6c7": {
    "tokens": 256,
    "content": "drawn from µ. We denote a specific dynamic Bayesian Stackelberg game as {R, Θ, {Cθ}θ∈Θ, µ, T}.\nThe leader plays a Dynamic Bayesian Stackelberg Policy 3 (DSP) π which specifies a leader strategy\nxt = π(jt−1) ∈ ∆m at each round t, where jt−1 = (j1, ··· , jt−1)4 is the follower’s past responses.\nThe leader commits to a DSP π before the game starts and the follower observes the policy in advance.\nThis commitment assumption is common in the literature, e.g. the dynamic pricing problem [ 23, 57],\ndynamic mechanism design [4, 42, 48] and Stackelberg security games [ 54]. We call the optimal DSP\nthe Dynamic Bayesian Stackelberg Equilibrium (DSE).\n2.1.3 Learning in Dynamic Bayesian Stackelberg Games\nThe leader in a dynamic Bayesian Stackelberg game is not primarily concerned with identifying the\nfollower type, as in traditional learning paradigms. Instead, the leader seeks to maximize her utility\nover the rounds of the game. However, to do this, the leader may take advantage of the follower’s",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_44",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9300661baf9c48640db9cfbdba94d3bc": {
    "tokens": 488,
    "content": "Proof. Proof. Consider the inner integral to equation 15:\nZ\nG(m,k)\nX\nC∈Fd(Hn)\n1 (C ∩ L ̸= {0})1B(C)ϕ∗(dL)\n=\nX\nC∈Fd(Hn)\n1 B(C)\nZ\nG(m,k)\n1(C ∩ L ̸= {0})ϕ∗(dL)\n=\nX\nC∈Fd(Hn)\n1 B(C)2Um−k,ϕ∗(C)\nTherefore,\nP(Ck\nn ∈ B) = 2\nC(n, k)\nZ\nG(m,m−1)\nX\nC∈Fd(Hn)\n·1 B(C)Um−k,ϕ∗(C)ϕn(d(Hn)) (17)\nFrom equations 17 and 14, for a non-negative and measurable function g on PCm, the following\nholds:\nE[g(Ck\nn)] = 2C(n, m)\nC(n, k) E[g(Sn)Um−k,ϕ∗(Sn)] (18)\nIf we choose g to be the identity function, then\nE[Um−k,ϕ∗(Sn)] = C(n, k)\n2C(n, m) (19)\nas desired.\nWe will use the following corollary and lemma in our proof of Theorem 3.\nCorollary 6. For a set of random half-spaces {H+\n1 , H+\n2 , ...,H+\nn } distributed according to ϕ that\nsatisfies Assumption 2, the probability that a hyperplane sampled from ϕ intersects the intersection\nof the half-spaces is:\nE\n\nχ\n\n \\\ni∈[n]\nH+\ni ∩ H′\n\n\n\n =\nPm−2\ni=0\n\u0000n−1\ni\n\u0001\nPm−1\ni=0\n\u0000n−1\ni\n\u0001 (20)\nProof. Proof. The corollary is a direct consequence of Corollary 5 with ϕ∗ = ϕ.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_131",
    "file_path": "CSE_1.pdf"
  },
  "chunk-0061f8474d8aef65376d0637bc6ab1a4": {
    "tokens": 262,
    "content": "C1 t0 t1 t2\nt0 0 1 0.5\nt1 0.5 0 0.5\nt2 0.5 1 0\nC2 t0 t1 t2\nt0 0 0.5 1\nt1 0.5 0 1\nt2 0.5 0.5 0\nTable 9: Utility Matrices for a Stackelberg Security Game with three follower types.\nThe leader has 1 unit of resource to protect three targets from the attacker whose type is drawn\nuniformly from {C0, C1, C2}. Each follower type Ci, i∈ {0, 1, 2} prefers target ti. Table 10 shows\nthe leader’s expected average utility in the dynamic setup for different numbers of interaction rounds.\nNote the leader’s expected average utility dropped overall compared to the result in the main paper,\nwhich is reasonable since the leader has to use the same resource to protect more targets.\nE.2 Additional Results on Structured Games\nIn Table 11, we provide further experimental results on smaller random game instances compared to\nthose discussed in the main paper. We continue to compare the runtime and average leader utility",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_175",
    "file_path": "CSE_1.pdf"
  },
  "chunk-9712fa84f9b89e8ef40a8b3a56202513": {
    "tokens": 365,
    "content": "Proposition 1 (Proposition A.1 [ 57]). The auction mechanism Mπ is feasible and its expected\nrevenue is equivalent to the dynamic pricing policy π’s average expected revenue over T.\nProof. Proof. To begin with, we formally define the auction mechanism Mπ’s allocation probability\nQM and payment PM with respect to a buyer report v.\nQM (v) =\nTX\nt=1\nj∗\nt (v)\nT\nPM (v) =\nTX\nt=1\nj∗\nt (v)π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\nT\nTo prove Mπ is a feasible auction mechanism, we prove it satisfies incentive compatibility, individual\nrationality, and QM (v) ≤ 1.\nFirst, because j∗\nt (v) ∈ {0, 1}, we have that QM (v) ≤ 1.\nNext, we prove the individual rationality of the mechanism, i.e. the buyer’s expected utility\nv · QM (v) − PM (v) ≥ 0. By definition,\nv · QM (v) − PM (v) = 1\nT\nTX\nt=1\nj∗\nt (v)\n\u0000\nv − π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\u0001\n. (4)\nIn other words, we need to prove PT\nt=1 j∗\nt (v)\n\u0000\nv −π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\u0001\n, which is exactly the buyer’s",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_118",
    "file_path": "CSE_1.pdf"
  },
  "chunk-5345efa17663cc846b647433b9232697": {
    "tokens": 349,
    "content": "Schneider [35].\nConsider the m-dimensional space, Rm and the m-dimensional unit sphere denoted by Sm−1.\nLet Dm be the set of closed, convex cones in Rm, and let PCm be the set of polyhedral cones in Rm\nwhere PCm ⊂ Dm. For all D ∈ Dd, the set K = D ∩Sm−1 is a convex body in Sm−1. We denote the\nset of convex bodies in Sm−1 as KS. We also denote by G(m, k) the Grassmanian of k-dimensional\nlinear subspaces of Rm.\nDefinition 6 (Generalized Spherical (j, ϕ)-Quermassintegral). Let ϕ∗(·) be a normalized measure\nover G(m, m− j) where j ∈ {0, ..., m}. Then the generalized spherical j-quermassintegral for\nK ∈ KS is:\nUj,ϕ∗ = 1\n2\nZ\nG(m,m−j)\nχ(K ∩ L)ϕ(dL). (11)\nWhere χ(·) is the Euler characteristic.\nNote that if ϕ∗ = νm−j, the normalized Haar measure on G(m, m− j), then the generalized\nspherical (j, νm−j)-quermassintegral is identical to the standard spherical j-quermassintegral. If K\nis a convex body in Sm−1 that is not a great sub-sphere, χ(K ∩ L) = 1 (K ∩ L ̸= ∅) for almost all",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_125",
    "file_path": "CSE_1.pdf"
  },
  "chunk-129a1dfb9b659d0944e36a3d3c0ac0c3": {
    "tokens": 280,
    "content": "auxiliary decision variables z and w. Specifically,\nzt,θ\njt =\ntY\nt′=1\nyθ\nt′,jt′ , ∀θ ∈ Θ, t∈ [T], jt ∈ [n]t,\nwt,θ\njt,i = xt\njt−1,i ·\ntY\nt′=1\nyθ\nt′,jt′ , ∀θ, i, t,jt ∈ [n]t.\n(52)\nThe key challenge for deriving our re-formulation is to set up the right set of constraints for\nthese new variables z, w so that they will exactly enforce the feasibility of the original variables.\nConstraints (45b) – (45c) in Program (45) serve the purpose, which leads to our following main\ntheorem and, consequently, a practical formulation for computing the DSE. Note that MILP (45) and\nMIP (47) themselves are not, in general, equivalent since not all feasible solutions of one correspond\nto feasible solutions of the other. However, we show that we can convert any optimal solution of\none program into a corresponding feasible optimal solution for the other. The proof is as follows.\nProof. Proof of the connection between MILP (45) and MIP (47). The proof has two steps. We first",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_166",
    "file_path": "CSE_1.pdf"
  },
  "chunk-2b8e510d6000853b8d54be82d0e3305e": {
    "tokens": 404,
    "content": "t=1 j∗\nt (v)\n\u0000\nv −π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\u0001\n, which is exactly the buyer’s\noptimal utility, is greater than or equal to 0. From the definition of j∗(v) in equation (3), this is\ntrivially satisfied, since 0 is the buyer’s utility when setting jt = 0, ∀t.\nFinally, we prove the incentive compatibility of the mechanism\nv · QM (v) − PM (v) ≥ v · QM (u) − PM (u). (5)\nIt is equivalent to prove\nTX\nt=1\nj∗\nt (v)\n\u0000\nv − π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\u0001\n≥\nTX\nt=1\nj∗\nt (u)\n\u0000\nv − π\n\u0000\nj∗\n1(u), ··· , j∗\nt−1(u)\n\u0001\u0001\n, (6)\nwhich is correct by the definition of j∗(v) from (3). As a result, we have proved that the mechanism\nMπ is a feasible mechanism.\nLast but not least, note that Mπ’s expected revenue is the expected payment,\nPM (v) = 1\nT\nTX\nt=1\nj∗\nt (v)π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\n(7)\nwhere PT\nt=1 j∗\nt (v)π\n\u0000\nj∗\n1(v), ··· , j∗\nt−1(v)\n\u0001\nis exactly the seller’s expected revenue when running dynamic\npolicy π for T rounds, proving the proposition.",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_119",
    "file_path": "CSE_1.pdf"
  },
  "chunk-b855d7e6c81e30a428d985e7d7d298a6": {
    "tokens": 303,
    "content": "Therefore, set T∗ such that:\nT∗ = max\n \n1\nV θ(x∗, j∗θ(x∗)) − maxj /∈BR(Θ′) V θ(x∗, j) + 1, 1\nV θ(x∗, j∗θ(x∗)) − maxj /∈BR(Θ′) V θ(x∗, j) + 2\n!\n,\nproves the theorem.\nB.2 Proof Details for Section 3.2\nIn this section, we will provide the details of the proofs of Theorem 3 and Corollary 1. To do so, we\nwill define some additional notation, and in doing this we will follow the conventions of Hug and\nSchneider [35]. A key step will be establishing Corollary 5, and the proof of this corollary is closely\nrelated, with some important modifications, to the proof of Corollary 4.2 in Hug and Schneider\n[35]. Therefore, Corollary 5 should be viewed as a corollary to Corollary 4.2 in Hug and Schneider\n[35]. However, we allow for a generalized notion of the quermassintegral in our derivation, which is\nessential to prove Corollary 5, making our result a strict generalization of Corollary 4.2 in Hug and\nSchneider [35].",
    "chunk_order_index": 0,
    "full_doc_id": "CSE_1.pdf_124",
    "file_path": "CSE_1.pdf"
  }
}